I",<h1 id="gpu">GPU</h1>

<p>GPU加速是通过大量线程并行实现的</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220114174216952.png" alt="image-20220114174216952" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220114174659499.png" alt="image-20220114174659499" /></p>

<p>GPU 和CPU通信 (GPU与CPU通过PCIe总线连接)</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220114175542810.png" alt="image-20220114175542810" /></p>

<h2 id="硬件资源">硬件资源：</h2>

<p><strong>Streaming multiprocessor: GPU大核</strong></p>

<p>流多处理器，多个SP+其他资源(warp scheduler, register, shared memory等)，</p>

<p><strong>Streaming processor/ CUDA core/核心</strong></p>

<p>流处理器, 基本计算单元，可以并行运算的单元，被分组为warp</p>

<p><strong>Wrap</strong></p>

<p>一个warp包含了多个32个整数倍的sp, Wrap是GPU调度单位，用一个warp中线程执行相同的指令，但是每个线程都包含自己的指令地址计数器和寄存器状态，也有自己独立的执行路径</p>

<h2 id="软件资源-cuda-program">软件资源 cuda program：</h2>

<p><strong>Grid</strong>:
多个block构成一个Grid</p>

<p><strong>Block</strong>
多个threads会被组成一个block，<strong>block内的线程共享内存通信</strong></p>

<p><strong>Thread</strong>
一个cuda的并行程序会被很多个threads执行</p>

<p>一个线程需要两个内置的坐标变量（blockIdx，threadIdx）来唯一标识</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220114190846959.png" alt="image-20220114190846959" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220114185208375.png?lastModify=1642158002" alt="image-20220114185208375" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220114174938201.png?lastModify=1642158002" alt="image-20220114185208375" /></p>

<p>kernel的这种线程组织结构天然适合vector,matrix等运算，如我们将利用上图2-dim结构实现两个矩阵的加法，每个线程负责处理每个位置的两个元素相加，代码如下所示。线程块大小为(16, 16)，然后将N*N大小的矩阵均分为不同的线程块来执行加法运算。</p>

<h1 id="cuda">CUDA</h1>

<h2 id="定义">定义</h2>

<p>A general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU.</p>

<p>CUDA是NVIDIA推出的用于自家GPU的<strong>并行计算</strong>框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。</p>

<h2 id="cuda-programming">CUDA programming</h2>

<p>在 CUDA 的架构下，一个<strong>程序</strong>分为两个部份：host 端和 device 端。Host 端是指在 CPU 上执行的部份，而 device 端则是在显示芯片上执行的部份。Device 端的程序又称为 “kernel”。通常 host 端程序会将数据准备好后，复制到显卡的内存中，再由显示芯片执行 device 端程序，完成后再由 host 端程序将结果从显卡的内存中取回。执行流程:</p>

<ol>
  <li>分配host内存，并进行数据初始化；</li>
  <li>分配device内存，并从host将数据拷贝到device上；</li>
  <li>调用CUDA的<strong>核函数</strong>在device上完成指定的运算；</li>
  <li>将device上的运算结果拷贝到host上；</li>
  <li>释放device和host上分配的内存。</li>
</ol>

<h2 id="内存模型">内存模型</h2>

<p>每个线程有自己的私有本地内存（Local Memory），而每个线程块有包含共享内存（Shared Memory）,可以被线程块中所有线程共享，<strong>其生命周期与线程块一致。</strong>此外，所有的线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory）。</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/v2-6456af75530956da6bc5bab7418ff9e5_720w.jpg" alt="img" /></p>

<h2 id="重要的apis">重要的APIs</h2>

<pre><code class="language-c">// 在device上申请一定字节大小的显存
cudaError_t cudaMalloc(void** devPtr, size_t size);
// 释放分配的内存使用cudaFree函数
cudaError_t cudaFree(void** devPtr);
//负责host和device之间数据通信的cudaMemcpy函数
cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)
// CUDA unified memory
cudaError_t cudaMallocManaged(void **devPtr, size_t size, 
  
</code></pre>

<p><strong>例子</strong></p>

<pre><code class="language-c">int main()
{
    int N = 1 &lt;&lt; 20;
    int nBytes = N * sizeof(float);
    // 申请host内存
    float *x, *y, *z;
    x = (float*)malloc(nBytes);
    y = (float*)malloc(nBytes);
    z = (float*)malloc(nBytes);

    // 初始化数据
    for (int i = 0; i &lt; N; ++i)
    {
        x[i] = 10.0;
        y[i] = 20.0;
    }

    // 申请device内存
    float *d_x, *d_y, *d_z;
    cudaMalloc((void**)&amp;d_x, nBytes);
    cudaMalloc((void**)&amp;d_y, nBytes);
    cudaMalloc((void**)&amp;d_z, nBytes);

    // 将host数据拷贝到device
    cudaMemcpy((void*)d_x, (void*)x, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy((void*)d_y, (void*)y, nBytes, cudaMemcpyHostToDevice);
    // 定义kernel的执行配置
    dim3 blockSize(256);
    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
    // 执行kernel
    add &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(d_x, d_y, d_z, N);

    // 将device得到的结果拷贝到host
    cudaMemcpy((void*)z, (void*)d_z, nBytes, cudaMemcpyDeviceToHost);

    // 检查执行结果
    float maxError = 0.0;
    for (int i = 0; i &lt; N; i++)
        maxError = fmax(maxError, fabs(z[i] - 30.0));
    std::cout &lt;&lt; "最大误差: " &lt;&lt; maxError &lt;&lt; std::endl;

    // 释放device内存
    cudaFree(d_x);
    cudaFree(d_y);
    cudaFree(d_z);
    // 释放host内存
    free(x);
    free(y);
    free(z);

    return 0;
}
</code></pre>

<p>单独在host和device上进行内存分配，并且要进行数据拷贝，这是很容易出错的。好在CUDA 6.0引入统一内存（<strong>Unified Memory</strong>）来避免这种麻烦，简单来说就是统一内存使用一个托管内存来共同管理host和device中的内存，并<strong>且自动在host和device中进行数据传输。CUDA中使用 cudaMallocManaged 函数分配托管内存</strong>， 利用统一内存，可以将上面的程序简化如下：</p>

<pre><code class="language-c">unsigned int flag=0);
int main()
{
    int N = 1 &lt;&lt; 20;
    int nBytes = N * sizeof(float);

    // 申请托管内存
    float *x, *y, *z;
    cudaMallocManaged((void**)&amp;x, nBytes);
    cudaMallocManaged((void**)&amp;y, nBytes);
    cudaMallocManaged((void**)&amp;z, nBytes);

    // 初始化数据
    for (int i = 0; i &lt; N; ++i)
    {
        x[i] = 10.0;
        y[i] = 20.0;
    }

    // 定义kernel的执行配置
    dim3 blockSize(256);
    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
    // 执行kernel
    add &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(x, y, z, N);

    // 同步device 保证结果能正确访问
    cudaDeviceSynchronize();
    // 检查执行结果
    float maxError = 0.0;
    for (int i = 0; i &lt; N; i++)
        maxError = fmax(maxError, fabs(z[i] - 30.0));
    std::cout &lt;&lt; "最大误差: " &lt;&lt; maxError &lt;&lt; std::endl;

    // 释放内存
    cudaFree(x);
    cudaFree(y);
    cudaFree(z);

    return 0;
}
</code></pre>

<p>相比之前的代码，使用统一内存更简洁了，值得注意的是<strong>kernel执行是与host异步的</strong>，由于<strong>托管内存自动进行数据传输</strong>，这里要用cudaDeviceSynchronize()函数保证device和host同步，这样后面才可以正确访问kernel计算的结果。</p>

<h2 id="cuda-initialization">CUDA initialization</h2>

<h3 id="作用"><strong>作用</strong>:</h3>

<p>其中一个就是创建 cuda context。即调用这些函数的时候，需要已经有context 存在了。cuda context 非常重要，它作为一个容器，管理了所有对象的生命周期，大多数的CUDA函数调用需要contex.</p>

<p>一个device 对应一个context，所有线程都可以使用。</p>

<h3 id="创建cuda-context"><strong>创建cuda context</strong></h3>

<p><strong>隐式调用 （cuda runtime 软件层的库， 是隐式调用）</strong></p>

<p>cuda runtime创建的context 是针对所有线程的，即一个device 对应一个context，所有线程都可以使用。</p>

<p>cuda runtime 不提供API直接创建CUDA context，而是通过lazy initialization。在<strong>调用每一个CUDART库函数</strong>时，它会检查当前是否有context存在，假如<strong>需要context，那么才自动创建</strong>。</p>

<p>cuda runtime将context和device的概念合并了，即在一个gpu上操作可看成在一个context下。因而cuda runtime提供的函数形式类似cudaDeviceSynchronize()而不是与driver API 对应的cuCtxSynchronize()。</p>

<p><strong>显式调用 （cuda driver API，驱动层的库，显式调用）</strong></p>

<p>cuda driver API 创建的context是针对一个线程的，<strong>即一个device，对应多个context，每个context对应多个线程，线程之间的context可以转移。</strong></p>

<p>在driver API中，<strong>每一个cpu线程必须要创建 context</strong>，或者从其他cpu线程转移context。如果没有context，就会报错。怎样才回到导致报错呢？即如果没有创建context，就直接调用 driver api创建上面那些对象，就会报错。因为上面的那些对象在runtime 和driver api 中都有函数可以创建。因此，注意注意！！！</p>

<p>每个cpu线程都有一个<strong>current context</strong>的栈，新建新的context就入栈。针对每一个线程只能有一个出栈变成可使用的current context，而这个游离的context可以转移到另一个cpu线程，通过函数cuCtxPushCurrent/cuCtxPopCurrent来实现。</p>

<p>当context被销毁，里面分配的资源也都被销毁，一个context内分配的资源其他的context不能使用。</p>

<p>注意：</p>

<p>1、隐式调用的context是primary context； 显示调用的context是standard context</p>

<p>2、每次cuda初始化比较费时间，其中一个工作可能就是使用runtime 进行了隐式调用context。因此，如果要避免这部分，有一个方法就是使用cudasetdevice 或者 cudaFree(0)  提前创建context。</p>

<p>The canonical way to force runtime API context establishment is to call <code>cudaFree(0)</code>. If you have multiple devices, call <code>cudaSetDevice()</code> with the ID of the device you want to establish a context on, then <code>cudaFree(0)</code> to establish the context.</p>

<h1 id="cudnn">CUDNN</h1>

<p>是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。如果你要用GPU训练模型，cuDNN不是必须的，但是一般会采用这个加速库</p>

:ET