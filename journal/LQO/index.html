<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link media="all" rel="stylesheet" type="text/css" href="https://nlgithubwp.github.io/css/pygments.css" />
   <meta property="og:url" content="https://nlgithubwp.github.io/journal/LQO/" />
   <link rel="canonical" href="https://nlgithubwp.github.io/journal/LQO/" />


  <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
<link rel="icon" href="/tech-notebook/favicon.ico?" type="image/x-icon">

    <meta name="description" content="It's the niceties that make the difference fate gives us the hand, and we play the cards.">

    <title>Lqo - NoteBook</title>

    <link rel="canonical" href="https://nlgithubwp.github.io/tech-notebook/journal/LQO/">

<!-- CSS -->
	
    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/tech-notebook/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/tech-notebook/css/main.css">
    <link rel="stylesheet" href="/tech-notebook/css/galleries.css">
     <link rel="stylesheet" href="/tech-notebook/css/super-search.css">
    <link rel="stylesheet" href="/tech-notebook/css/magnificpopup.css">
    <link rel="stylesheet" href="/tech-notebook/css/pygments.css">

    <!-- Custom Fonts -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Josefin+Sans:400,600&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->


</head>


<head>

</head>

<body>

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top" style="margin-right:1em; z-index: 1;">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <i class="fa fa-bars fa-lg"></i>
            </button>
            <a href="/tech-notebook/" title="HOME"><img src="/tech-notebook/img/logos/logo6.png" alt="logo" style="width:150px;cursor:pointer;margin-top:15px;"/></a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right" style="margin-right:1em; text-shadow: -.025em -.025em 0 rgba(0, 133, 161, 0.75), .025em -.025em 0 rgba(0, 133, 161, 0.75), -.025em .025em 0 rgba(0, 133, 161, 0.75), .025em .025em 0 rgba(0, 133, 161, 0.75);">
               
          
            
            
              <li><a href="https://nlgithubwp.github.io/tech-notebook/gallery/">GALLERY</a></li>
            
          
        
          
            
            
              <li><a href="https://nlgithubwp.github.io/tech-notebook/journal/">JOURNAL</a></li>
            
          
        
          
            
            
              <li><a href="https://nlgithubwp.github.io/tech-notebook/about/">ABOUT</a></li>
            
          
        
          
            
            
              <li><a href="https://nlgithubwp.github.io/tech-notebook/resume/">CV</a></li>
            
          
        
                            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>



    <html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        
        <meta name="viewport" content="width=device-width">

<link href="/tech-notebook/css/tags-wrap.css" rel="stylesheet" />
  
    </head>
    <body>
  



<article>
<!-- Post Header -->
<header class="intro-header" style="background-image: url('/tech-notebook/')">
    <div class="container">
        <div class="row">
            <div class="post-heading">
                    <h1 style="color: #acd578;">Lqo</h1>
                    
                    <span class="meta">Posted on August 8, 2024</span>

<span class="time-tag-categ" style="font-size:.7em;background-color: rgba(128, 128, 128, 0.45);">


<i class="fa fa-clock-o"></i>&nbsp;
30 minute read



 


      
                    
   </span>

            </div>
        </div>
    </div>
</header>

<!-- Post Content -->

    <div class="container">
        <div class="row">
            

             <h1 id="some-ideas">Some Ideas</h1>

<p>Non-parametric model for model selection for query opt?</p>

<p>Dirichlet Process Mixture: It solves the problem of determining the number of mixture components by allowing the data to drive the complexity of the model.</p>

<h2 id="settings">Settings</h2>

<table>
  <thead>
    <tr>
      <th>config</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>shared_buffers &gt;= N * work_mem</td>
      <td>16GB</td>
    </tr>
    <tr>
      <td>work_mem (sort/hash operation)</td>
      <td>4GB</td>
    </tr>
    <tr>
      <td>effective_cache_size (affects whether to use index scans or sequential scans.)</td>
      <td>16GB</td>
    </tr>
    <tr>
      <td>temp_buffers</td>
      <td>4GB</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>geqo</td>
      <td>off</td>
    </tr>
    <tr>
      <td>AUTOVACUUM</td>
      <td>off</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>enable_bitmapscan</td>
      <td>on</td>
    </tr>
    <tr>
      <td>enable_tidscan</td>
      <td>on</td>
    </tr>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>max_parallel_workers</td>
      <td>4</td>
    </tr>
    <tr>
      <td>max_parallel_workers_per_gather</td>
      <td>4</td>
    </tr>
    <tr>
      <td>max_worker_processes</td>
      <td>4</td>
    </tr>
  </tbody>
</table>

<h1 id="templates">Templates</h1>

<h2 id="takeawaysproblems">TakeawaysProblems</h2>

<h2 id="assumptions">Assumptions</h2>

<h2 id="techniques">Techniques</h2>

<h3 id="query-type">Query Type</h3>

<h3 id="query-encoding">Query Encoding</h3>

<h3 id="model">Model</h3>

<h2 id="workloads">Workloads</h2>

<h3 id="datasets">Datasets</h3>

<h3 id="dynamic-workload">Dynamic Workload</h3>

<h1 id="1-dsb-datasets">1. DSB datasets</h1>

<p>Existing benchmarks lack of</p>

<ol>
  <li>more distinct query instances</li>
  <li>dynamic workloads</li>
  <li>guidance of comprehensively comparing the performance trade-offs for workload-driven and traditional database systems.</li>
</ol>

<h2 id="shift">Shift</h2>

<p>Skew</p>

<ul>
  <li>Both exponential / Zipfian distributions are for those where the common value is more frequent</li>
  <li>The exponential distribution is often used for modeling <strong>time-related data</strong> or “waiting times,” while Zipfian is used to model scenarios where there is a <strong>ranking or ordering of items</strong>, such as popularity.Zipfian</li>
</ul>

<h2 id="workloads-1">Workloads</h2>

<ol>
  <li>
    <p>skews and correlations, skews on individual columns, correlations between columns in the same table, and correlations of columns across multiple tables.</p>

    <p><strong>=&gt; more skews and correlations to individual columns and multiple columns within a table and across tables.</strong></p>
  </li>
  <li>
    <p>more join patterns: including non-equi joins and cyclic join graphs.</p>

    <p><strong>=&gt; new query templates to enrich the join patterns.</strong></p>
  </li>
  <li>
    <p>slice data at a fine granularity with multiple predicate filters on table columns.</p>

    <p><strong>=&gt; fine-grained data slicing with additional predicate filters.</strong></p>
  </li>
  <li>
    <p><strong>=&gt; configurable parameterization to generate queries.</strong></p>
  </li>
  <li>
    <p><strong>=&gt; comprehensive user guidance.</strong></p>
  </li>
</ol>

<h2 id="experiments">Experiments</h2>

<h1 id="2-modeling-shifting-workloads-for-learned-database-systems">2. Modeling Shifting Workloads for Learned Database Systems</h1>

<h2 id="takeaways">Takeaways</h2>

<h2 id="problems">Problems</h2>

<p>The key question is when and how best to re-train the model.</p>

<ol>
  <li>
    <p>Retraining within near queries =&gt; catastrophic forgetting, large queries =&gt; time-consuming.</p>
  </li>
  <li>
    <p>Workload imbalance leads to poor performance.</p>
  </li>
</ol>

<h2 id="assumptions-1">Assumptions</h2>

<p>Assumes the <strong>underlying data</strong> to be static.</p>

<h2 id="techniques-1">Techniques</h2>

<h3 id="query-type-query-encoding">Query Type, Query Encoding</h3>

<p>As in other papers</p>

<h3 id="model-1">Model</h3>

<p>distributed shift detector (per l query) =&gt; train a new model/retrain the existing model with the importance of past/current queries.</p>

<ul>
  <li>KS test for measuring the distribution of each feature of two samples.</li>
  <li>Bonferroni correction to adjust the significance level.</li>
</ul>

<p>online clustering algorithm (Bayesian non-parametric models) =&gt; managing the replay buffer =&gt; providing data to re-train.</p>

<ul>
  <li>cluster based on the query feature encoding, other than learned embedding or gradients.</li>
  <li>propose DP-Medoides based K-Medoids, since it is more robust to outliers/supports customerize distnace computes.</li>
  <li>manage the reply buffer such that its balance, diversified.</li>
</ul>

<p>re-training: combined loss of pass and current queries.</p>

<h2 id="workloads-2">Workloads</h2>

<h3 id="datasets-1">Datasets</h3>

<p>IMDB with JOB</p>

<p>DSB (50GB)</p>

<p>the query join graph and/or the distribution of query <strong>predicates</strong> may shift with time.</p>

<h3 id="dynamic-workload-1">Dynamic Workload</h3>

<ol>
  <li>Generate a query pool with each query classes having 200 queries.</li>
  <li>shifting workloads:
    <ol>
      <li>randomly pick three classes (fixed)</li>
      <li>for each class, randomly pick 100 queries</li>
      <li>concatenate those queries.</li>
      <li>repeate from 2-4 to form a workload with queries A(100), B(100), C(100), A(100)…</li>
    </ol>
  </li>
  <li>measure performance after retraining and evaluate in other classes (except the current one).</li>
</ol>

<h1 id="3-balsa">3. Balsa</h1>

<h2 id="takeaways-1">Takeaways</h2>

<p>Neo-impl is still not robust enough to generalize to unseen test queries and suffers from high variance.</p>

<p>Balsa’s better generalization is due to a <strong>broader state coverage</strong> offered by simulation, <strong>on-policy learning</strong>, and safe exploration</p>

<p>the main point of this paper is to learn without expert demonstrations.</p>

<h2 id="problems-1">Problems</h2>

<p>Existing RL-based methods assume the availability of a mature query optimizer to learn from.</p>

<h2 id="assumptions-2">Assumptions</h2>

<p>the <strong>database content is kept static</strong>.</p>

<p>Updates to the schema, appends, or in-place updates can be handled by retraining.</p>

<p>This assumption implies that the agent need not solve a learning problem with a shifting distribution. Another assumption is that Balsa currently optimizes select-project-join (SPJ) blocks.</p>

<h2 id="techniques-2">Techniques</h2>

<h3 id="formulation">Formulation</h3>

<p>query optimization as an MDP:</p>

<ul>
  <li>state: partial query plan.</li>
  <li>action step in building a query plan in a <strong>bottom-up fashion</strong>， either join two tables, or assign a scan method.</li>
  <li>a reward is given only at the final (terminal) state, each action only gets zero rewards</li>
</ul>

<h3 id="query-type-1">Query Type</h3>

<h3 id="query-encoding-1">Query Encoding</h3>

<p>same as NEO</p>

<h3 id="process">Process</h3>

<p>learn to optimize queries without learning from an existing expert optimizer</p>

<p>Simulation and reality phases:</p>

<ul>
  <li>simulation phase trains a value network based on the simple <strong>CE cost</strong>, although this <strong>CE cost</strong> is not accurate, it enables the agent to avoid the worst plans.</li>
  <li>reality phase finetunes the value network.</li>
  <li>during exploration, Balsa generates several best-predicted plans (instead of the best) and then picks the best unseen one out of them.</li>
</ul>

<p>Generalization:</p>

<ul>
  <li>train with high diversity, merge experiences with several independently agent with different seeds.</li>
</ul>

<p>Some techniques</p>

<ul>
  <li>timeouts,</li>
  <li>exploration only on the best few plans.</li>
  <li>
    <p>on-policy learning better than retraining.</p>
  </li>
  <li></li>
</ul>

<h2 id="workloads-3">Workloads</h2>

<p>TPCH</p>

<h1 id="4-is-your-learned-query-optimizer-behaving-as-you-expect-a-machine-learning-perspective">4. Is Your Learned Query Optimizer Behaving As You Expect? A Machine Learning Perspective</h1>

<h2 id="takeaways-2">Takeaways</h2>

<p>A number of join is an irrelevant proxy for execution time.</p>

<p>GEQO is used for large join number queries, which can be disabled to fairly compare.</p>

<h2 id="problems-2">Problems</h2>

<p>The absence of a <strong>unified reproducible framework</strong> make it currently impossible to fairly compare the results of LQOs.</p>

<p>Many filter combinations result in the same selectivity. Hence, the model would potentially suffer from the mismatch between features and target variables and will only perform well if this inconsistency is mitigated.</p>

<h2 id="assumptions-3">Assumptions</h2>

<h2 id="techniques-3">Techniques</h2>

<h3 id="query-type--query-encoding">Query Type &amp; Query Encoding</h3>

<p>All existing LQO are query-driven methods other than data-driven.</p>

<p>Encoding schema for a query should be both <strong>expressive and robust</strong>.</p>

<ul>
  <li>roubust: unique 1-to-1 mapping between the feature variables and the latency or cost of a query and its given plan</li>
  <li>expressive: the encoding should clearly reflect both the global and local context.</li>
</ul>

<p>Bao does not use talbes but only cardinalities and costs.</p>

<p>=&gt; easier to retrain and schema-agnosticism.</p>

<p>=&gt; not one-to-one mapping: different filters in a query has same cardinalities for the same table. ??</p>

<p>Bao deos not use query encoding =&gt; increases the probability of converging to a local optimum.</p>

<h3 id="model-2">Model</h3>

<p>An architecture that chains the ML models like in HybridQO, where different target variables are served to different models in the ML pipeline</p>

<h2 id="workloads-4">Workloads</h2>

<h3 id="dataset">Dataset</h3>

<p>JOB and STACK</p>

<p>We do not use the STATS-CEB benchmark, as it was originally developed for <strong>challenges</strong> in cardinality estimation as opposed to end-to-end query optimization, which is the focus of this paper.</p>

<h3 id="dynamic-workload-2">Dynamic Workload</h3>

<p>four worklods</p>

<ul>
  <li>Leave One Out Sampling.</li>
  <li>Random Sampling.</li>
  <li>Base Query Sampling.</li>
  <li>Covariate Shift on JOB</li>
</ul>

<h1 id="5-neo">5. NEO</h1>

<h2 id="takeaways-3">Takeaways</h2>

<h2 id="problems-3">Problems</h2>

<p>However, none demonstrate that their improved cardinality estimations actually lead to better query plans.</p>

<p>Furthermore, unlike <strong>join order selection</strong>, selecting <strong>join operators</strong> (e.g., hash join, merge join) and choosing <strong>indexes</strong> cannot be entirely reduced to cardinality estimation.</p>

<h2 id="assumptions-4">Assumptions</h2>

<p>existence of a Sample Workload consisting of queries representative of the user’s total workload</p>

<p>existence of the underlying engine’s capabilities (i.e., exercising a representative set of operators).</p>

<h2 id="techniques-4">Techniques</h2>

<p>end-to-end learning approach, including join order, index, and physical operator selection.</p>

<h3 id="formulation-1">Formulation</h3>

<p>query optimization as an MDP:</p>

<ul>
  <li>state: partial query plan.</li>
  <li>action step in building a query plan in a <strong>bottom-up fashion</strong></li>
  <li>a reward is given only at the final (terminal) state, each action only gets zero rewards</li>
</ul>

<h3 id="query-type-2">Query Type</h3>

<h3 id="query-encoding-2">Query Encoding</h3>

<p>Query level information (<strong>join graph</strong>) + plan level information (<strong>join order</strong>).</p>

<ul>
  <li>query-level:
    <ul>
      <li>treats the <strong>join graph as an adjacency matrix</strong> and only encodes the <strong>upper triangular portion</strong> into one dim vec.</li>
      <li>column predicates vector: 1-hot, hist, row-vector. row vector is to represent each row as a sentence.</li>
    </ul>
  </li>
  <li>plan-level
    <ul>
      <li>plan as a tree, each node is a vector of size J+2R, J is a number of join types, and R is a number of relations. Each relation has two values: table use or not? scan type</li>
    </ul>
  </li>
</ul>

<h3 id="model-3">Model</h3>

<p>Value Model (tree-convolution) to predict the final execution time for a <strong>partial or complete plan</strong>.</p>

<ul>
  <li>value network does not predict the cost of a subplan, but rather the best possible latency achievable from an execution plan that includes a given subplan.</li>
</ul>

<p>best-first search + value network =&gt; valuate iteration techniques to generate the final plan.</p>

<ul>
  <li>init-state: every scan is unspecified and there are no joins.</li>
  <li>
    <p>action: either (1) fusing together two roots with a join operator or (2) turning an unspecified scan into a table or index scan</p>
  </li>
  <li>not greedily follow the suggestions of the value network.</li>
  <li>with bootstrapping (warm-ups), it can avoid sample inefficiency problem</li>
</ul>

<p>retraining from scratch other than fine-tuning.</p>

<h2 id="workloads-5">Workloads.</h2>

<h3 id="datasets-2">Datasets</h3>

<p>JOB, TPCH, Corp</p>

<h3 id="dynamic-workload-3">Dynamic Workload</h3>

<p>Train and test on various templates.</p>

<h1 id="6lero">6.Lero</h1>

<h2 id="takeaways-4">Takeaways</h2>

<p>The hint set-based strategy has some intrinsic limitations</p>

<h2 id="problems-4">Problems</h2>

<p>existing methods are</p>

<ul>
  <li>unstable performance.</li>
  <li>high learning costs.</li>
  <li>slow model updating.</li>
</ul>

<p>It is unncessary to estimate cost with model.</p>

<h2 id="assumptions-5">Assumptions</h2>

<p>We assume <strong>constant resource budget per plan execution in this paper</strong></p>

<h2 id="techniques-5">Techniques</h2>

<p>Plan explorer + model two-class classification.</p>

<h3 id="formulation-2">Formulation</h3>

<p>pair-wise classification problem</p>

<h3 id="query-type-3">Query Type</h3>

<h3 id="query-encoding-3">Query Encoding</h3>

<p>each node in the tree: join method, scane method, cardinality, row wideth, table used. (No estimated costs)</p>

<h3 id="model-4">Model</h3>

<p>Pre-train on the synthetic workloads based on the <strong>estimated cost</strong> in the postgresql.</p>

<ul>
  <li>randomly generate a number of plans with different join orders and predicates on different tables,</li>
  <li>randomly set the cardinality for each sub-plan of each plan and feed them into the native cost model (without executing the plans) to derive the estimated plan costs as labels.</li>
</ul>

<p>Online training</p>

<ul>
  <li>get the real execution latency and re-train the model.</li>
  <li>retrian every 100 queries on IMDB and STATS, every 30 queries on TPC-H and every 50 queries on TPC-DS.</li>
</ul>

<h2 id="workloads-6">Workloads.</h2>

<h3 id="datasets-3">Datasets</h3>

<p>tpch, tpc-ds, stats, imdb.</p>

<h3 id="dynamic-workload-4">Dynamic Workload</h3>

<p>“time series split” strategy, always evaluated on the new template.</p>

<h1 id="7leon">7.LEON</h1>

<h2 id="takeaways-5">Takeaways</h2>

<p>Theoretically, as long as the <strong>cardinality estimations and the cost model</strong> are accurate, this architecture obtains the optimal query plan</p>

<p>The db query optimizer can do transformation for logical expression, such as</p>

<ul>
  <li>unnest an in/exists subquery to a semi-join query.</li>
  <li>subquery pushing involves <strong>evaluating a subquery earlier</strong> in the execution process and “pushing” its results into the main query. This changes the order of operations to reduce computational cost or improve query performance.</li>
</ul>

<p>Those rules cannot be used in ML-based method, and thus ML-basd method cannot conduct the plan enumeration completely.</p>

<p>DB can be extendend with new operators, while the ML-based model can only learn in trail-and-error manner.</p>

<p>cost model to initialize the model =&gt; cold start problem.</p>

<h2 id="problems-5">Problems</h2>

<p>ML-based methods learn domain-specifc knowledge in a data-driven manner. Thus, it suffers from the following two fundamental limitations:</p>

<ul>
  <li>only for spj query and cannot handle complicated query.</li>
  <li>cold-start problem</li>
</ul>

<p>Therefore, ML should aid the traiditonal query optimizer otherthan replacing it. And ML model should start from the traditional query optimizer.</p>

<h2 id="assumptions-6">Assumptions</h2>

<h2 id="techniques-6">Techniques</h2>

<p>start from current performance and self-adjust to a certain dataset or workload.</p>

<ul>
  <li>
    <p>cost estimation as a contextual parise-ranking problem.</p>
  </li>
  <li>
    <p>robust plan exploration strategy</p>

    <ul>
      <li>a plan with a higher ranking position should be explored with a higher probability</li>
      <li>learned optimizer should correct its errors that lead to sub-optimal query plans</li>
    </ul>
  </li>
</ul>

<h3 id="formulation-3">Formulation</h3>

<p>formalizes query optimization as a contextual pair-wise plan ranking problem</p>

<h3 id="query-type-4">Query Type</h3>

<h3 id="query-encoding-4">Query Encoding</h3>

<p>logical properties: output cardinality and join graph of the sub query.</p>

<p>physical properties: plan tree and the physical property of the whole plan. Each node is one-hot encoding representing the physical operators and sort order.</p>

<h3 id="model-5">Model</h3>

<p>Intra-equivalent set Model: estimate the cost for each plan and uncertainty.</p>

<p>Inter-equivalent set Model: it is used to prune the redundant search space.</p>

<h2 id="workloads-7">Workloads.</h2>

<h3 id="datasets-4">Datasets</h3>

<h3 id="dynamic-workload-5">Dynamic Workload</h3>

<h3 id="performance">Performance</h3>

<p>training efficiency and latency performance.</p>

<h1 id="8-base">8. BASE</h1>

<h2 id="takeaways-6">Takeaways</h2>

<p>transfer the model pre-trained on the cost and then to the latency sounds promising.</p>

<p>learn a mapping from cost to latency sounds promising.</p>

<h2 id="problems-6">Problems</h2>

<p>The DBMS cost model can estimate the cost for an execution plan, which is expected to reflect the latency of the plan, but they are not positively correlated in practice</p>

<h2 id="assumptions-7">Assumptions</h2>

<h2 id="techniques-7">Techniques</h2>

<p>Two stage RL-based framework.</p>

<p>Stage 1: fit RL to chose the operation with leatest cost.</p>

<p>Stage 2: transfer a <strong>pre-trained policy based on the cost</strong> to a <strong>better policy</strong> based on latency</p>

<p>leverage cost as a trade-off to increase training efficiency and then transfer the pre-trained model based on the cost to a new model that can adapt to latency signals</p>

<h3 id="formulation-4">Formulation</h3>

<p>BASE formulates the procedure of locating and filling the gap between cost and latency as a variant of IRL.</p>

<h3 id="query-type-5">Query Type</h3>

<p>SPJ</p>

<h3 id="query-encoding-5">Query Encoding</h3>

<p>Same as NEO</p>

<h3 id="model-6">Model</h3>

<p>In the 2nd stage, we use a calibrated reward function by applying a parameterized calibration function to the cost signals.</p>

<p>calibration function only makes the calibrated cost signals and the latency signals more correlated</p>

<h2 id="workloads-8">Workloads.</h2>

<h3 id="datasets-5">Datasets</h3>

<p>STATS, IMDB.</p>

<h3 id="dynamic-workload-6">Dynamic Workload</h3>

<p>None</p>

<h3 id="performance-1">Performance</h3>

<p>Fast training</p>

<h1 id="9reinforcement-learning-with-tree-lstm-for-join-order-selection">9.Reinforcement Learning with Tree-LSTM for Join Order Selection</h1>

<h2 id="takeaways-7">Takeaways</h2>

<p>This handle the schema change by using dynamic pooling to map anylengthh input to a fixed sized vector.</p>

<h2 id="problems-7">Problems</h2>

<p>Existing work</p>

<ol>
  <li>fixed-length feature vectors cannot capture the structural information of a join tree, which may produce poor join plans.</li>
  <li>Moreover, it may also cause retraining the neural network when handling schema changes (e.g., adding tables/columns) or multialias table names that are common in SQL queries.</li>
</ol>

<h2 id="assumptions-8">Assumptions</h2>

<h2 id="techniques-8">Techniques</h2>

<p>Two stage RL-based framework.</p>

<p>Stage 1: pre-train the Deep Reinforcement Learning with cost,</p>

<p>Stage 2: finetune the model with the latency.</p>

<h3 id="formulation-5">Formulation</h3>

<p>BASE formulates the procedure of locating and filling the gap between 푟휙and 푟(푙 )as a variant of IRL.</p>

<h3 id="query-type-6">Query Type</h3>

<p>SPJ</p>

<h3 id="query-encoding-6">Query Encoding</h3>

<p>query  + data</p>

<ul>
  <li>query: contains join information in a query.
i.e., matrix to represent all join relation, then flatten into one vector. Then it applies FC to get the final representation</li>
  <li>columns:
    <ul>
      <li>numertical value: vector of 4 value, if column is in join, is =? is&gt;? is&lt;?</li>
      <li>string value: only use the selectivity information and get the column representions.</li>
    </ul>
  </li>
  <li>table: concatenate all columns representions.</li>
  <li>join tree: use tree-LSTM to learn a representation.
    <ul>
      <li>combine both Child-Sum Tree-LSTM and N-ar Tree LSTM.</li>
    </ul>
  </li>
</ul>

<h3 id="model-7">Model</h3>

<p>Representation: use a Tree-LSTM</p>

<p>RL: use DQN.</p>

<ul>
  <li>cost training and latency tunning.</li>
  <li>not move target from cost to latecy directly, but instead treate them as multi task learning, where train two Q network but use the same input representation.</li>
</ul>

<p>Handle the dynamic worklaod:</p>

<ul>
  <li>use dynamic pooling to encoding any dynamic length query</li>
  <li>Adding columns also use dynamic pooling</li>
</ul>

<h2 id="workloads-9">Workloads.</h2>

<h3 id="datasets-6">Datasets</h3>

<p>tpch, imdb</p>

<h3 id="dynamic-workload-7">Dynamic Workload</h3>

<p>insert/delete columns, add new tables etc.</p>

<h3 id="performance-2">Performance</h3>

<h1 id="10theoretical-analysis-of-learned-database-operations-under-distribution-shift-through-distribution-learnability">10.Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability</h1>

<h2 id="takeaways-8">Takeaways</h2>

<h2 id="problems-8">Problems</h2>

<p>no theoretical work shows any advantage in using the learned methods in dynamic datasets and under distribution shift.</p>

<p>The goal of this paper is to theoretically understand the capabilities of learned models for database operations, show why and when they outperform non-learned alternatives and provide theoretical guarantees on their performance</p>

<h2 id="assumptions-9">Assumptions</h2>

<h2 id="techniques-9">Techniques</h2>

<p>indexing, cardinality estimation and sorting when presence of insertions from a possibly changing data distribution</p>

<h3 id="formulation-6">Formulation</h3>

<p>data after inserting n times is D_n</p>

<p>its distribution is learnable with parameter T, S, B.</p>

<ul>
  <li>T: the model can be evaluted in T operatios.</li>
  <li>take S to store.</li>
  <li>n*B to learn the model.</li>
</ul>

<h1 id="11eraser-eliminating-performance-regression-on-learned-query-optimizer">11.Eraser: Eliminating Performance Regression on Learned Query Optimizer</h1>

<h2 id="takeaways-9">Takeaways</h2>

<p>This is mainly focus on the dataset side other than model side, for any trained model, it filter the search spcae based on model performance to increate the effectiveness of the model.</p>

<h2 id="problems-9">Problems</h2>

<p>learned model has regression due to reasons like</p>

<ul>
  <li>inherent difficulty of the learning problem,</li>
  <li>the low generalization ability of the prediction model on new data,</li>
  <li>the under-fitting on training data due to insufficient training data,</li>
  <li>loss of features, noisy labels and inappropriate model training methods.</li>
</ul>

<h2 id="assumptions-10">Assumptions</h2>

<p>Learned model has low generalizatin ability, thus has big regression problem.</p>

<h2 id="techniques-10">Techniques</h2>

<p>Two stage RL-based framework.</p>

<p>Stage 1: coarse-grained filter to remove highly risky plans</p>

<ul>
  <li>Insight: ML prediction model is likely to underperform due to encountering <strong>unseen feature values</strong>, therefore <strong>unexpected plan space</strong> into <strong>subspaces</strong> based on unseen feature values. this is like data labelling process.</li>
  <li>train a model to predict <strong>precise</strong> or <strong>imprecise</strong> of each subspace.</li>
</ul>

<p>Stage 2: cluster plans in fine-grained manner, evalutes each cluster based on the prediction result.</p>

<ul>
  <li>Cluster the remaining precise plans into groups based on their feature values and the model’s reliability.</li>
  <li>Assign each cluster a <strong>reliability interval</strong></li>
</ul>

<h3 id="formulation-7">Formulation</h3>

<p>A performance elimination method perf_elim such that benefits are maximum and loss is minimized.</p>

<h3 id="query-type-7">Query Type</h3>

<p>SPJ</p>

<h3 id="query-encoding-7">Query Encoding</h3>

<p>plan-level features: join relations, filtering predicates and operator types</p>

<ul>
  <li>join: join type, scan type,</li>
  <li>filter: low and high value</li>
  <li>relation: 0/1 to indicate existance.</li>
  <li>structure: use a categorical variable to indicates it, bushy tree, left-deep and right-deep, on plans joining 4 tables</li>
</ul>

<p>data-level features: estimated cardinality and data distribution</p>

<h3 id="model-8">Model</h3>

<p>unexpected plan explorer filter bad plans based on the <strong>relability threshold.</strong></p>

<p>cluster the remining plans and use the <strong>segment model</strong>, and also assign <strong>relability value</strong> to reflect the quality.</p>

<p>Then filter again, and use <strong>plan selection method</strong> to balance the benefit of <strong>improvements</strong> and the <strong>risk of regression</strong>.</p>

<h2 id="workloads-10">Workloads.</h2>

<h3 id="datasets-7">Datasets</h3>

<p>tpch, imdb</p>

<h3 id="dynamic-workload-8">Dynamic Workload</h3>

<p>trian on 25% etc and evaluate on the fixed test dataset.</p>

<h3 id="performance-3">Performance</h3>

<h1 id="12access-path-selection-in-a-relational-database-management-system">12.Access Path Selection in a Relational Database Management System</h1>

<h2 id="takeaways-10">Takeaways</h2>

<p>This handle the schema change by using dynamic pooling to map anylengthh input to a fixed sized vector.</p>

<h2 id="problems-10">Problems</h2>

<p>This paper will address the issues of access path selection for queries</p>

<ul>
  <li>single table queries</li>
  <li>join</li>
  <li>nested queries</li>
</ul>

<h2 id="techniques-11">Techniques</h2>

<p>query processing are 4 phases:</p>

<ul>
  <li>parsing:</li>
  <li>optimization:</li>
  <li>code generation:</li>
  <li>execution:</li>
</ul>

<h1 id="13cardbench-a-benchmark-for-learned-cardinality-estimation-in-relational-databases">13.CardBench: A Benchmark for Learned Cardinality Estimation in Relational Databases</h1>

<h2 id="takeaways-11">Takeaways</h2>

<p>learn from the transferable features, and not from the workload specific features such as table and attribute names.This approach enables learning of zero-shot CE models.</p>

<h2 id="problems-11">Problems</h2>

<p>Traditional CE techniques used in modern database systems have well-known limitations, as they <strong>make simplistic data modeling assumptions, such as data uniformity and independence of columns in the tables</strong>.</p>

<p>Learned CE have not been adapted in practice, due to their high training overheads.</p>

<h2 id="techniques-12">Techniques</h2>

<p>zero-shot models are pre-trained on a <strong>broad spectrum of different datasets</strong> and use <strong>transferable features (dataset-agnostic)</strong>, such as table sizes, allowing them to generalize to unseen datasets.</p>

<h3 id="query-type-8">Query Type</h3>

<p>SPJ</p>

<h3 id="model-9">Model</h3>

<p>GNN model to encode the query structure.</p>

<p>MLP-based cardinality model to get the final prediction.</p>

<h2 id="workloads-11">Workloads.</h2>

<h3 id="datasets-8">Datasets</h3>

<p>tpch, imdb</p>

<h3 id="dynamic-workload-9">Dynamic Workload</h3>

<h3 id="performance-4">Performance</h3>

<h1 id="14loger-a-learned-optimizer-towards-generating-efficient-and-robust-query-execution-plans">14.LOGER: A Learned Optimizer towards Generating Efficient and Robust Query Execution Plans</h1>

<h2 id="takeaways-12">Takeaways</h2>

<p>This work aims to change the search space compared with bao, but still use hint, it input action(disable a join algorithm) and input join order into the network to predict the latency.</p>

<h2 id="problems-12">Problems</h2>

<h2 id="assumptions-11">Assumptions</h2>

<h2 id="techniques-13">Techniques</h2>

<p><strong>reward weighting</strong> to reduce the <strong>impact of previous poor operators</strong> and make LOGER pay less attention to disastrous plans by log transformation.</p>

<h3 id="formulation-8">Formulation</h3>

<p>RL to build a tree</p>

<p>state: subplan’s join order and operator</p>

<p>action: join table and with restricted operator (disable a join algorithm, like disable hash join)</p>

<h3 id="query-type-9">Query Type</h3>

<p>spj without sub-queries</p>

<h3 id="query-encoding-8">Query Encoding</h3>

<p>Graph Transformer: GT not only efficiently captures relationships between table representations that include both table and predicate information, but also integrates global structural information of join graph into table representations.</p>

<ul>
  <li>table: table-level learned embedding vector.</li>
  <li>column: column-level statistic vector with column type, proportion of unique values, unique value count, null values, index.</li>
  <li>predicts: join exist or not, selectivity (via approximation of DBMS cardinality estimator.)</li>
</ul>

<h3 id="model-10">Model</h3>

<p>The value model <strong>evaluates state-action pair</strong> candidates of subplans by predicting the reachable lowest latency among all plans that can be reached from the pair.</p>

<p>state network:</p>

<p>action network: take all join as input,</p>

<h2 id="workloads-12"><strong>Workloads</strong>.</h2>

<h3 id="datasets-9">Datasets</h3>

<h3 id="dynamic-workload-10">Dynamic Workload</h3>

<p>change schema.</p>

<h3 id="performance-5">Performance</h3>

<p>2 hours of training, 2x speedup</p>

<h1 id="15prestos-history-based-query-optimizer">15.Presto’s History-based Query Optimizer</h1>

<h2 id="takeaways-13">Takeaways</h2>

<p>share-everything =&gt; latency over scalability.</p>

<h2 id="problems-13">Problems</h2>

<p>learning based CE/cost method requires large trainng efforts, less robustness, and hard to debug.</p>

<h2 id="techniques-14">Techniques</h2>

<p>HBO tracks q<strong>uery execution statistics</strong> at the operator node, and <strong>uses those to predict future performance</strong> for <strong>similar</strong> queries.</p>

<ol>
  <li>Similary query fetching: only record the template,</li>
  <li>HBO is more powerful than CBO as it can store various runtime statistics related to scheduling as well</li>
</ol>

<ul>
  <li>join reordering: HBO record the stats for some join order, and thus accurate. If not meet before, we use the CBO.</li>
</ul>

<h1 id="16parqo-penalty-aware-robust-plan-selection-in-query-optimization">16.PARQO: Penalty-Aware Robust Plan Selection in Query Optimization</h1>

<h2 id="takeaways-14">Takeaways</h2>

<p>This paper mentioned that some CE is sensitive while other are not.</p>

<p>The whole paper based on the probability of f(s∣s′):</p>

<ul>
  <li>It starts as an error distribution estimated from historical data.</li>
  <li>It drives sensitivity analysis by identifying critical dimensions.</li>
  <li>It enables robust plan selection by sampling likely true selectivities and guiding the optimizer to focus on realistic scenarios.</li>
</ul>

<h2 id="problems-14">Problems</h2>

<p>Accurate estimation of the cardinality and cost comes at the cost of runtime monitoring and ongoing maintenance.</p>

<p>Therefore, how much influence of such uncertainty bring to the selectivity estimates still underexplored.</p>

<p>Robustness is tied to the uncertainty, and use the uncertainty in practise has challenges:</p>

<ul>
  <li>errors distributed on data and query worklaod is not well utiized in exsting work.</li>
  <li>complex query has large cardinality estimate spaces, and thus has overhead to evaluate all other competing plans.</li>
  <li>support learned optimzier at scale and in efficient way.</li>
</ul>

<h2 id="assumptions-12">Assumptions</h2>

<h2 id="techniques-15">Techniques</h2>

<table>
  <tbody>
    <tr>
      <td>getting the distribution of f(s</td>
      <td>s`), where s` is estimated cardinalities, and s is the true selectivities.</td>
    </tr>
  </tbody>
</table>

<p>based on the distribution, find the <strong>sensitive</strong> dimensions for a given query plan got from estimates s`.</p>

<ul>
  <li>identify the dimensions with biggest impact on the user-defined penalty function.</li>
  <li>using sobol method</li>
</ul>

<p>build a pool of candidate plan by sampling from the distribution of true selectivities conditioned on their estimates, then select one with lowest penalty.</p>

<ol>
  <li>
    <p>Error profling:</p>

    <table>
      <tbody>
        <tr>
          <td>build a model to estimate  f(s</td>
          <td>s`) given Q and s`</td>
        </tr>
      </tbody>
    </table>

    <p>break the query into querylet, which is a subquery pattern. like join with local selection.</p>

    <p>for each querlet in (single-table querylet, two table querley, three-table querylet with fixed pattern), measure the true cardinalities and estimated cardinalities.</p>

    <p>one model for low-selectivity, one model for hight selectivity.</p>
  </li>
  <li>
    <p>Sensitive analysis</p>

    <p>it uses <strong>sobol sensitiity</strong> to identify the sensitive selectivity dimensions for a given plan.</p>
  </li>
  <li>
    <p>Plan selection
use sampling to sample a pool of plans based on the probability f(s, s`),
estimate the cost</p>
  </li>
</ol>

<h3 id="formulation-9">Formulation</h3>

<p>No</p>

<h3 id="query-type-10">Query Type</h3>

<p>No</p>

<h3 id="query-encoding-9">Query Encoding</h3>

<p>No</p>

<h3 id="model-11">Model</h3>

<p>KDE</p>

<h2 id="workloads-13"><strong>Workloads</strong>.</h2>

<h3 id="datasets-10">Datasets</h3>

<p>IMDB, DSB, STATS</p>

<h3 id="dynamic-workload-11">Dynamic Workload</h3>

<p>split database into multiple instance in time series manner.</p>

<h3 id="performance-6">Performance</h3>

<h3 id="implementation-details">Implementation details</h3>

<p>ok, so from the beginning of the execution, i think this is the overall logic</p>

<ol>
  <li>it pre-defines some query templates, and then run lots of queries and collect 
(target dimension (which is only base table + number of join between any two pairs), estimate seleivity, and real seleivity.)</li>
  <li>given a query, it will first fit a kde model for the error of each target dimension based on this datasets, and get a list of kde for each target dimension, then it sample 1000 points and get mean selectivity error value as the so called real error.</li>
  <li>it then perform sensitive analysis. it first get the all predefined possible sensitive dims, named as pre_defined_querylet_dims,which may not exactly the same the target dimensions. (from the paper the thoery, if the dimensions in the target dimention has no selection, then we don;t care it, it’s CE in pg is accurate)</li>
  <li>It get optimizaion plan fisrt (this is by feeding the optimzier with the real cardinalities and selectivility), 
then it get the hint to generate this plan in later. 
in the adjust_selectivity process, it first update selective of the the target_dimensions and update by provided error_samples, which can be inaccurate, since it is to measure the influence, is it?
And then it updates all others dimensiton which is not in the target_dimensions but in the sampled_real_sel_err_mean_list, 
each key in the sampled_real_sel_err_mean_list is actually the predefined possible sensitives, we make all those who are not in the target as accurate as possible, so it will not casuing any influence in later. 
therefore, those dimens are update based on the sampled_real_sel_err_mean_list, while the target is based on the error_samples.
finally all changes are updated to the updated_join_sel, which is a litst of original q_card_info.est_join_sel and note, this incldue all the join imforamtions, not one-to-one join, but one-to-multiple joins.
here is a questions, the code after “what the fk is this” feels like has not effetc, is it?</li>
  <li>it also get the default execution plan, which is the postgresql plan</li>
  <li>then it generate sampels from joint error dist , name is joint error dist, but actually it sampled indenpendently. gfinally it get A and B sampels.</li>
  <li>then it generates a problem, which is basiclaly for the sobol.</li>
  <li>then i think i’m not familiart with the implementatin of the sobol, what is tht following do? in the theory, i think i just need to estimate ethe improtance of reach dimension by computing s = var()/Var(Y), is it? and we use MC sampling to do that, so we need A, B , and mc smapling also change the computing method of s. i only know htose</li>
</ol>

<h1 id="17plaque-automated-predicate-learning-at-query-time">17.PLAQUE: Automated Predicate Learning at Query Time</h1>

<h2 id="takeaways-15">Takeaways</h2>

<p>Discovering the proper predicates is also important to improve the efficiency.</p>

<h2 id="problems-15">Problems</h2>

<p>Predicates pushdown is useful in speeding up the query processing, but query optimzier only pushdown the predictes exist in the query. However, tables with no predicated also has a chance to prune.</p>

<p>Therefore, this paper try to discover the predicates and add that to the query in query execution.</p>

<h1 id="18-fastgres-making-learned-query-optimizer-hinting-effective">18. FASTgres: Making Learned Query Optimizer Hinting Effective</h1>

<h2 id="takeaways-16">Takeaways</h2>

<p>This paper learn multiple GB models for each context, reduing the model complexity and training inefficiency. But not quite handle the change of data or query ? data is not changed in this setting.</p>

<h2 id="problems-16">Problems</h2>

<p>bao cannot achieve the potetial to the full extent.</p>

<h2 id="assumptions-13">Assumptions</h2>

<h2 id="techniques-16">Techniques</h2>

<p>Divide and conquer approach.</p>

<ul>
  <li>workload -&gt; partitioned into query groups, each with different join tables and join predicates -&gt; context.</li>
  <li>learn a separate and context-sensitive ML model for each context.
here since the partition is based on join pattern, join is not required to learn, we can only learn to map the filter predicates to the hint set.</li>
</ul>

<h3 id="formulation-10">Formulation</h3>

<p>Divide and conquer approach.</p>

<h3 id="query-type-11">Query Type</h3>

<h3 id="query-encoding-10">Query Encoding</h3>

<p>predicate-based encoding</p>

<h3 id="model-12">Model</h3>

<p>GB models</p>

<h2 id="workloads-14"><strong>Workloads</strong>.</h2>

<h3 id="datasets-11">Datasets</h3>

<p>Stack [18], JOB [13], and TPC-H</p>

<h3 id="dynamic-workload-12">Dynamic Workload</h3>

<p><strong>randonmly</strong> sample training query</p>

<h3 id="performance-7">Performance</h3>

<h1 id="19os-pre-trained-transformer-predicting-query-latencies-across-changing-system-contexts">19.OS Pre-trained Transformer: Predicting Query Latencies across Changing System Contexts</h1>

<h2 id="takeaways-17">Takeaways</h2>

<p>This paper propose to train a generatl embedding to ecndoe the informatin of the system envs, such as cpu, memory etc, and finally combine the result of this and instance-specifc embedding together to predict the final result of the task.</p>

<h2 id="problems-17">Problems</h2>

<p>Model leared in one context fail when tested on a new system context. Thus, there is a generalization gap.</p>

<p>Therefore, existing work cannot utilize the interaction of different resources to accurately predict query performance.</p>

<h2 id="assumptions-14">Assumptions</h2>

<p>system state does not change drastically just before and during query execution.</p>

<h2 id="techniques-17">Techniques</h2>

<h3 id="formulation-11">Formulation</h3>

<p>query plan, OS logs -&gt; query latency.</p>

<h3 id="query-type-12">Query Type</h3>

<h3 id="query-encoding-11">Query Encoding</h3>

<p>GCN for query plans, the model should produce an embedding vector with all query specifc properties to predict the latency.</p>

<h3 id="model-13">Model</h3>

<ol>
  <li>
    <p>Database specifialized query plan model: encoding the plan into a vecotr. like using <strong>GCN</strong>.</p>
  </li>
  <li>
    <p><strong>A universal (i.e., applicable to any workload) transformer model</strong> which takes as input recent OS logs, and produces an embedding vector that captures the system state.</p>
  </li>
  <li>Embedding vectors are used as input to a universal Multi Layer Perceptron (MLP) module to produce the latency prediction.</li>
  <li>When in new datasets, it fix the Osprey model and prediction head, and only update the query embedding model, the GCN.</li>
</ol>

<h2 id="workloads-15"><strong>Workloads</strong>.</h2>

<h3 id="datasets-12">Datasets</h3>

<p>150k PostgreSQL query execution plans and corresponding linux system logs for over 2500 unique queries from 14 database workloads executed on 10 AWS instance types, and under various system loads</p>

<h3 id="dynamic-workload-13">Dynamic Workload</h3>

<h3 id="performance-8">Performance</h3>

<h1 id="20-zero-shot-cost-models-for-out-of-the-box-learned-cost-prediction">20. Zero-Shot Cost Models for Out-of-the-box Learned Cost Prediction</h1>

<h2 id="takeaways-18">Takeaways</h2>

<h2 id="problems-18">Problems</h2>

<p>training data collection is high cost.</p>

<p>simply model often under/over-estimates the true costs of queries, since they cannot capture complex interactions in the query plan and data.</p>

<h2 id="assumptions-15">Assumptions</h2>

<p>we only focus on the transfer of <strong>learned cost models</strong> across databases for a <strong>single database system on a fixed hardware.</strong></p>

<h2 id="techniques-18">Techniques</h2>

<p>New query and data representation that allows zero-shot cost models to be pre-trained across databases.</p>

<h3 id="formulation-12">Formulation</h3>

<p>The goal of <strong>zero-shot cost estimation</strong> is to predict query latencies (i.e., runtimes) on an <strong>unseen database</strong> without having observed any query on this <strong>unseen database</strong>.</p>

<h3 id="query-type-13">Query Type</h3>

<h3 id="query-encoding-12">Query Encoding</h3>

<p>A new representation of <strong>queries</strong> that can generalize across databases.</p>

<p>Encoded information:</p>

<ul>
  <li>physical plan operators as nodes.</li>
  <li>input columns, tables, predicate information.</li>
</ul>

<p>Encoded in transferable way:</p>

<ul>
  <li>predicate structure: data types of columns/operators, intermediate cardinalities</li>
</ul>

<h3 id="model-14">Model</h3>

<p>a database-agnostic model to <strong>estimate the runtime cost</strong> and a databasedependent model (e.g., histograms) to capture data characteristics.</p>

<p>database-agnostic cost model: feed with both estimated cardinalities and output of database-dependent model, it generates a cost.</p>

<h2 id="workloads-16"><strong>Workloads</strong></h2>

<h3 id="datasets-13">Datasets</h3>

<h3 id="dynamic-workload-14">Dynamic Workload</h3>

<h3 id="performance-9">Performance</h3>

<h1 id="21-rethinking-learned-cost-models-why-start-from-scratch">21. Rethinking Learned Cost Models: Why Start from Scratch?</h1>

<h2 id="takeaways-19">Takeaways</h2>

<h2 id="problems-19">Problems</h2>

<p>end to end learning based cost model is hard to interpret and use, and retraining is costly.</p>

<ul>
  <li>learning-based models are trained on a specific database with specific hardware and software configurations. retraining is costly.</li>
  <li>data collection is time-consuming and expensive process.</li>
  <li>no insight of how the cost estimate was generated compared with the rules-based cost model.</li>
</ul>

<p>learned cost model should be easy to train, hight transferability, and interpretabe.</p>

<h2 id="assumptions-16">Assumptions</h2>

<p>we assume that the formula-based cost model can provide a good estimation, if all R-params in Table 1 are set correctly.</p>

<h2 id="techniques-19">Techniques</h2>

<p>Two stage offlien training stage and online refinement stage.</p>

<p>Static and Dynamic C parameters:</p>

<ul>
  <li>hardware, OS, database can be <strong>static configuration parameters.</strong></li>
  <li>data type and column correlation are dynamic configuration parameters.</li>
</ul>

<p>Data collection:</p>

<ul>
  <li>Execute queries on different CPU, memory, SSDs, HDDs, operating systems, etc</li>
</ul>

<p>Offline phase: capature relations between hyperparamers and static configuration parameters.</p>

<ul>
  <li>pre-train a decision tree for each of 8 operator, there are totally 8 trees.</li>
</ul>

<p>Online phase: adaptively tune each tree for the dynamic query/data factors.</p>

<ul>
  <li>hundreds of dynamic configuration parameters, such as query related, data related and 97 configuration parameters.</li>
  <li>online phase invoked when the cost model cannot provide a precise estimation for more than K queries.</li>
  <li><strong>few-show recommendatin model</strong> to rank all candidate parameters based on the effects on cost estimations.</li>
</ul>

<h3 id="formulation-13">Formulation</h3>

<p>based on the system and workload configutaion, it learn a mapping from the those configutaion to the hyperparameter of the cost function, i.e., determine optimal choices of R-params in the built-in cost formulas of major database.</p>

<p>G: C -&gt; R</p>

<p>The total operators considered are 8.</p>

<h3 id="query-type-14">Query Type</h3>

<h3 id="query-encoding-13">Query Encoding</h3>

<h3 id="model-15">Model</h3>

<p>decision tree model.</p>

<h2 id="workloads-17"><strong>Workloads</strong></h2>

<h3 id="datasets-14">Datasets</h3>

<p>IMDB, TPC-H and TPC-DS</p>

<h3 id="dynamic-workload-15">Dynamic Workload</h3>

<h3 id="performance-10">Performance</h3>

<p>###</p>


                <br>
          <br>
    <br>


<!--               <div class="post-author text-center">                       
	        <img src="/tech-notebook/img/" alt="'s photo" itemprop="image" class="post-avatar img-circle img-responsive"/>    
	    <h4>By <span itemprop="name" class="fn"><a href="/tech-notebook/about/" title="About " itemprop="url"></a></span></h4>
	    <div style="font-family: Open Sans, Helvetica Neue, Helvetica, Arial, sans-serif; display:inline-block;font-size:.75em;"></div>
</div> -->
    <hr class="small">
 <div class="post-share text-center">
    <p class="light small">
        <h6>END OF POST</h6>
    </p>
    <ul class="social-mini">

    </ul>
</div>
                        

                        
                <br>

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/tech-notebook/journal/219/" data-toggle="tooltip" data-placement="top" title="Distribution Shifts">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/tech-notebook/journal/DBInternal/" data-toggle="tooltip" data-placement="top" title="Dbinternal">Next Post &rarr;</a>
                    </li>
                    
                </ul>

        </div>
    </div>
</article>

<br>

<hr class="medium">

            <div class="tags-wrap">
   <div class="tags">

  <div class="section-heading group" style="color: #fff; text-align: center; margin-top: 10px;">

    <h3>Tags Cloud</h3>

  </div>










  <div class="tag-list">
    
  </div>

</div>

<hr class="medium">

<div class="categories">

  <div class="section-heading group" style="color: #fff; text-align: center; margin-top: 10px;">

    <h3>Categories Cloud</h3>

  </div>

 









 <div class="category-list">
   
<a href="/tech-notebook/journal/category//">  </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/a-paper-note/"> A paper note </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/cmu-database/"> CMU database </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/coding/"> coding </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/devices/"> devices </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/distributed-database/"> distributed database </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/machine-learning-basic/"> machine learning basic </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/operation-system/"> operation system </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/practise/"> practise </a> &nbsp;&nbsp;

<a href="/tech-notebook/journal/category/programming-language/"> programming language </a> &nbsp;&nbsp;

  </div>

</div>

<hr class="medium">

<br>
<br>

       </div>


<!-- jQuery -->    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
  <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
<!-- Custom Theme JavaScript -->
<script src="/tech-notebook/js/main.min.js "></script>
<!-- include image popups -->
<script src="/tech-notebook/js/jquery.magnific-popup.js"></script>

<script type="text/javascript">
      $(document).ready(function($) {
        $('a.popup').magnificPopup({
         type: 'image',
	  gallery:{
         enabled:true,
         navigateByImgClick: true,
         preload: [0,1] // Will preload 0 - before current, and 1 after the current image
       },
image: {
      titleSrc: function(item) {
              return item.el.attr('title') + '&nbsp;' + item.el.attr('data-caption');
            }
        }
          // other options
      });
});
    </script>

<script src="/tech-notebook/js/retina.min.js"></script>
<!-- include Masonry -->
<script src="/tech-notebook/js/isotope.pkgd.min.js"></script> 
<!-- include mousewheel plugins -->
<script src="/tech-notebook/js/jquery.mousewheel.min.js"></script>
<!-- include carousel plugins -->
<script src="/tech-notebook/js/jquery.tinycarousel.min.js"></script>
<!-- include svg line drawing plugin -->
<script src="/tech-notebook/js/jquery.lazylinepainter.min.js"></script>
<!-- include custom script -->
<script src="/tech-notebook/js/scripts.js"></script>
<!-- Modernizr -->
 <script src="/tech-notebook/js/modernizr.js"></script>

</body>
</html>






    <a href="javascript:void(0)" title="SEARCH" onclick="superSearch.toggle()" class="super-search-btn">
<span class="fa fa-search" style="font-size:1em; position:fixed; right: .75em; top: 1em; z-index: 2;"></span>
</a>
<div class="super-search" id="js-super-search">
<a href="javascript:void(0)" onclick="superSearch.toggle()" class="super-search__close-btn"">
<span class="fa fa-close" style="font-size:48px;"></span>
</a>
<input type="text" placeholder="Type here to search" class="super-search__input" id="js-super-search__input">
<ul class="super-search__results" id="js-super-search__results"></ul>
</div>



 <script src="/tech-notebook/js/super-search.js"></script>

<script>
superSearch({
    searchFile: '/tech-notebook/feed.xml',
    searchSelector: '#js-super-search', // CSS Selector for search container element.
    inputSelector: '#js-super-search__input', // CSS selector for <input>
    resultsSelector: '#js-super-search__results' // CSS selector for results container
});
</script>

    
        <div class="row"  style="background-color: #530720; padding-bottom: 40px; margin-top: 50px; margin-bottom: 80px;">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

      <center><h4><p>It's the niceties that make the difference fate gives us the hand, and we play the cards.</h4></center>


<!--<form style="padding:3px;text-align:center;font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:12px;" action="https://tinyletter.com/photorgasms" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/yourproject', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true"><input type="text" placeholder="Enter your email address"  style="width:200px;height:30px;border:0;background-color:#5A0F28;color:#dddddd;outline:0;padding-left:12px;" name="email" id="tlemail" />&nbsp; &nbsp; <input type="submit" value="Get notified!" style="background-color:#470118;color:#dddddd;height:30px;border:0;" /></form>-->
            </div>
        </div>
     
<style type="text/css">
        .tlemail::-webkit-input-placeholder {
   color: rgba(255,255,255,.45);
}
       .tlemail:-moz-placeholder { /* Firefox 18- */
   color: rgba(255,255,255,.45);  
}

      .tlemail::-moz-placeholder {  /* Firefox 19+ */
   color: rgba(255,255,255,.45);  
}

      .tlemail:-ms-input-placeholder {  
   color: rgba(255,255,255,.45);  
}
    </style>


    <!-- Footer -->
<footer>
<div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

                <ul class="list-inline text-center">

<!--                    -->
<!--                    <li>-->
<!--                        <a href="https://facebook.com/" data-toggle="tooltip" title="Facebook">-->
<!--                            <span class="fa-stack fa-lg">-->
<!--                                <i class="fa fa-circle fa-stack-2x"></i>-->
<!--                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>-->
<!--                            </span>-->
<!--                        </a>-->
<!--                    </li>-->
<!--                    -->
<!--                    -->
<!--                    <li>-->
<!--                        <a href="https://twitter.com/" data-toggle="tooltip" title="Twiiter">-->
<!--                            <span class="fa-stack fa-lg">-->
<!--                                <i class="fa fa-circle fa-stack-2x"></i>-->
<!--                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>-->
<!--                            </span>-->
<!--                        </a>-->
<!--                    </li>-->
<!--                    -->
<!--                    -->
<!--                    <li>-->
<!--                        <a href="https://instagram.com/" data-toggle="tooltip" title="Instagram">-->
<!--                            <span class="fa-stack fa-lg">-->
<!--                                <i class="fa fa-circle fa-stack-2x"></i>-->
<!--                                <i class="fa fa-instagram fa-stack-1x fa-inverse"></i>-->
<!--                            </span>-->
<!--                        </a>-->
<!--                    </li>-->
<!--                    -->
<!--                    -->
<!--                    <li>-->
<!--                        <a href="https://flickr.com/photos/" data-toggle="tooltip" title="Flickr">-->
<!--                            <span class="fa-stack fa-lg">-->
<!--                                <i class="fa fa-circle fa-stack-2x"></i>-->
<!--                                <i class="fa fa-flickr fa-stack-1x fa-inverse"></i>-->
<!--                            </span>-->
<!--                        </a>-->
<!--                    </li>-->
<!--                    -->
<!--                    -->
<!--                    <li>-->
<!--                        <a href="http://.deviantart.com">-->
<!--                            <span class="fa-stack fa-lg" data-toggle="tooltip" title="Deviantart">-->
<!--                                <i class="fa fa-circle fa-stack-2x"></i>-->
<!--                                <i class="fa fa-deviantart fa-stack-1x fa-inverse"></i>-->
<!--                            </span>-->
<!--                        </a>-->
<!--                    </li>-->
<!--                    -->
                    
                    <li>
                        <a href="https://github.com/NLGithubWP/tech-notebook" data-toggle="tooltip" title="Github">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
<!--            <li>-->
<!--    <a href="mailto:" data-toggle="tooltip" target="_blank" title="Email">-->
<!--                       <span class="fa-stack fa-lg">-->
<!--                                <i class="fa fa-circle fa-stack-2x"></i>-->
<!--                                <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>-->
<!--</span>-->
<!--          </a> </li>-->

<!--         <li>-->
<!--                        <a href="/tech-notebook/feed.xml" data-toggle="tooltip" title="Feed">-->
<!--                            <span class="fa-stack fa-lg">-->
<!--                                <i class="fa fa-circle fa-stack-2x"></i>-->
<!--                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>-->
<!--                            </span>-->
<!--                        </a>-->
<!--                    </li>-->
                </ul>
                <p class="copyright text-muted">Copyright &copy; NoteBook 2025</p>
<center><h6><p>Powered by <a href="https://github.com/jekyll/">Jekyll</a>.</p></h6></center>
            </div>
        </div>
    </div>
</footer>


   <script>
//** jQuery Scroll to Top Control script- (c) 
//** v1.1 (April 7th, 10')
//** 1) Adds ability to scroll to an absolute position (from top of page) or specific element on the page instead.
//** 2) Fixes scroll animation not working in Opera. 
var scrolltotop={
//startline: Integer. Number of pixels from top of doc scrollbar is scrolled before showing control
//scrollto: Keyword (Integer, or "Scroll_to_Element_ID"). How far to scroll document up when control is clicked on (0=top).
setting: {startline:100, scrollto: 0, scrollduration:1000, fadeduration:[500, 100]},
controlHTML: '<img src="https://cloud.githubusercontent.com/assets/14811095/13691821/88412468-e744-11e5-8bb5-94340afd92e7.png" style="filter:alpha(opacity=100); -moz-opacity:1;"/>', //HTML for control, which is auto wrapped in DIV w/ ID="topcontrol"
controlattrs: {offsetx:35, offsety:60}, //offset of control relative to right/ bottom of window corner
anchorkeyword: '#top', //Enter href value of HTML anchors on the page that should also act as "Scroll Up" links
state: {isvisible:false, shouldvisible:false},
scrollup:function(){
if (!this.cssfixedsupport) //if control is positioned using JavaScript
this.$control.css({opacity:0}) //hide control immediately after clicking it
var dest=isNaN(this.setting.scrollto)? this.setting.scrollto : parseInt(this.setting.scrollto)
if (typeof dest=="string" && jQuery('#'+dest).length==1) //check element set by string exists
dest=jQuery('#'+dest).offset().top
else
dest=0
this.$body.animate({scrollTop: dest}, this.setting.scrollduration);
},
keepfixed:function(){
var $window=jQuery(window)
var controlx=$window.scrollLeft() + $window.width() - this.$control.width() - this.controlattrs.offsetx
var controly=$window.scrollTop() + $window.height() - this.$control.height() - this.controlattrs.offsety
this.$control.css({left:controlx+'px', top:controly+'px'})

},

togglecontrol:function(){
var scrolltop=jQuery(window).scrollTop()
if (!this.cssfixedsupport)
this.keepfixed()
this.state.shouldvisible=(scrolltop>=this.setting.startline)? true : false
if (this.state.shouldvisible && !this.state.isvisible){
this.$control.stop().animate({opacity:1}, this.setting.fadeduration[0])
this.state.isvisible=true
}
else if (this.state.shouldvisible==false && this.state.isvisible){
this.$control.stop().animate({opacity:0}, this.setting.fadeduration[1])
this.state.isvisible=false
}

},
init:function(){
jQuery(document).ready(function($){
var mainobj=scrolltotop
var iebrws=document.all

mainobj.cssfixedsupport=!iebrws || iebrws && document.compatMode=="CSS1Compat" && window.XMLHttpRequest //not IE or IE7+ browsers in standards mode
mainobj.$body=(window.opera)? (document.compatMode=="CSS1Compat"? $('html') : $('body')) : $('html,body')
mainobj.$control=$('<div id="topcontrol">'+mainobj.controlHTML+'</div>')
.css({position:mainobj.cssfixedsupport? 'fixed' : 'absolute', bottom:mainobj.controlattrs.offsety, right:mainobj.controlattrs.offsetx, opacity:0, cursor:'pointer'})
.attr({title:'Scroll To Top'})
.click(function(){mainobj.scrollup(); return false})
.appendTo('body')

if (document.all && !window.XMLHttpRequest && mainobj.$control.text()!='') //loose check for IE6 and below, plus whether control contains any text
mainobj.$control.css({width:mainobj.$control.width()}) //IE6- seems to require an explicit width on a DIV containing text
mainobj.togglecontrol()

$('a[href="' + mainobj.anchorkeyword +'"]').click(function(){
mainobj.scrollup()

return false
})

$(window).bind('scroll resize', function(e){

mainobj.togglecontrol()

})

})

}

}

scrolltotop.init()
</script>  

</body>

</html>
