<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NoteBook</title>
    <description>It's the niceties that make the difference fate gives us the hand, and we play the cards.</description>
    <link>https://nlgithubwp.github.io/tech-notebook/</link>
    <atom:link href="https://nlgithubwp.github.io/tech-notebook/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 22 Feb 2025 10:24:33 +0000</pubDate>
    <lastBuildDate>Sat, 22 Feb 2025 10:24:33 +0000</lastBuildDate>
    <generator>Jekyll v3.9.5</generator>
    
      <item>
        <title></title>
        <description>&lt;p&gt;&lt;img src=&quot;../../img/a_img_store/image-20240604104259675.png&quot; alt=&quot;image-20240604104259675&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This paper assumes&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are at most 30 nodes in a tree.&lt;/li&gt;
  &lt;li&gt;There are at most 3 predictions for each column in a node.&lt;/li&gt;
  &lt;li&gt;Only support operators such as&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper solves the problem of&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;existing encoding methods do not fully utilize readily available database statistics in the representation, which characterizes the data distribution.&lt;/li&gt;
  &lt;li&gt;They typically have difficulty in modeling long paths of information flow in a query plan.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Contribution:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tree-structured Transformer model to learn the representation of physical query plans. Thus &lt;strong&gt;Improves the generalization of the learned representation.&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;Capture node dependencies via self-attention.&lt;/li&gt;
      &lt;li&gt;Integrate histograms into plan encoding.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;examine in cost estimation, cardinality estimation, index selection, and steering query optimizer.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Insights: Sec 4.2.1&lt;/p&gt;

&lt;p&gt;‘one-hot’ encoding 
(1) The dimension of the one-hot vector can easily blow up for large database schema.&lt;/p&gt;

&lt;p&gt;(2) It is difficult to handle new categorical variables when the database updates. For example, when new columns are added, the encoding scheme has to reset to the required dimensions, and thus we have to retrain the whole machine-learning system.&lt;/p&gt;

&lt;p&gt;To address these limitations, we propose to use a fixed-size, dense embedding to represent each categorical variable. These embeddings will be learned automatically through back-propagation when a machine-learning model is trained for a specific database task, such as cost estimation. As such, the important features that are relevant to the database task could be reflected in the embedding space [10].&lt;/p&gt;

&lt;p&gt;This setting also naturally supports inserting new variables. To insert a new category (e.g., a new table) to the representation model, we simply add a new random-initialized embedding for the variable, without affecting other learned embeddings.&lt;/p&gt;

&lt;p&gt;Details:&lt;/p&gt;

&lt;p&gt;Node:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;type-&amp;gt; em0.   Join -&amp;gt; em1,   column -&amp;gt;em2,   table -&amp;gt; em3.&lt;/p&gt;

    &lt;p&gt;cat(em0, em1, em2, em3) =&amp;gt; final em&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Node + hight e&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Position -&amp;gt; Embeddings.
position_embd + atree_atten_bias.&lt;/li&gt;
  &lt;li&gt;Hight -&amp;gt; Embeddings.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 22 Feb 2025 10:24:33 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/2024-06-04-220/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/2024-06-04-220/</guid>
        
        
      </item>
    
      <item>
        <title></title>
        <description>&lt;p&gt;It enhances the TPC-DS benchmark with complex data distribution and challenging yet semantically meaningful query templates&lt;/p&gt;

&lt;p&gt;the joins in the TPC-DS benchmark are mostly between a primary key and a foreign key or between a primary key and another primary key.&lt;/p&gt;

&lt;p&gt;Needs&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;distinct query instances&lt;/li&gt;
  &lt;li&gt;the distribution of query templates&lt;/li&gt;
  &lt;li&gt;more evaluation metrics&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;the paper preserves the realistic database schema from TPC-DS, and it enriches the TPC-DS benchmark in a number of ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;more skews and correlations to individual columns and multiple columns within a table and across table&lt;/li&gt;
  &lt;li&gt;new and semantically meaningful query templates (non primary-key-foreign-key)&lt;/li&gt;
  &lt;li&gt;fne-grained data slicing. This also increases the total number of distinct query instances&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, the DSB benchmark enhances the data generation of the TPC-DS benchmark with additional skews and correlations&lt;/p&gt;

</description>
        <pubDate>Sat, 22 Feb 2025 10:24:33 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/2024-05-30-217/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/2024-05-30-217/</guid>
        
        
      </item>
    
      <item>
        <title>Dbinternal</title>
        <description>&lt;h1 id=&quot;structure&quot;&gt;Structure&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tcop&lt;/code&gt; (Top-level Control Program)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postgres.c&lt;/code&gt;&lt;/strong&gt;: main entry point for query execution. It handles the query lifecycle:
accepts the query from the client (via the frontend-backend protocol), and coordinates the entire process:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Parses the query using the parser.&lt;/li&gt;
      &lt;li&gt;Uses the planner to generate an execution plan.&lt;/li&gt;
      &lt;li&gt;Sends the plan to the executor for actual execution.&lt;/li&gt;
      &lt;li&gt;Returns the results to the client.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;some-important-variables&quot;&gt;Some important variables&lt;/h1&gt;

&lt;p&gt;A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QueryDesc&lt;/code&gt; structure represents the state and details of the query being executed. It contains information like the query &lt;strong&gt;plan, execution context, snapshots, and parameters.&lt;/strong&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/DBInternal/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/DBInternal/</guid>
        
        
      </item>
    
      <item>
        <title>Lqo</title>
        <description>&lt;h1 id=&quot;some-ideas&quot;&gt;Some Ideas&lt;/h1&gt;

&lt;p&gt;Non-parametric model for model selection for query opt?&lt;/p&gt;

&lt;p&gt;Dirichlet Process Mixture: It solves the problem of determining the number of mixture components by allowing the data to drive the complexity of the model.&lt;/p&gt;

&lt;h2 id=&quot;settings&quot;&gt;Settings&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;config&lt;/th&gt;
      &lt;th&gt;size&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;shared_buffers &amp;gt;= N * work_mem&lt;/td&gt;
      &lt;td&gt;16GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;work_mem (sort/hash operation)&lt;/td&gt;
      &lt;td&gt;4GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;effective_cache_size (affects whether to use index scans or sequential scans.)&lt;/td&gt;
      &lt;td&gt;16GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;temp_buffers&lt;/td&gt;
      &lt;td&gt;4GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;geqo&lt;/td&gt;
      &lt;td&gt;off&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AUTOVACUUM&lt;/td&gt;
      &lt;td&gt;off&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;enable_bitmapscan&lt;/td&gt;
      &lt;td&gt;on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;enable_tidscan&lt;/td&gt;
      &lt;td&gt;on&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;max_parallel_workers&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;max_parallel_workers_per_gather&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;max_worker_processes&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;templates&quot;&gt;Templates&lt;/h1&gt;

&lt;h2 id=&quot;takeawaysproblems&quot;&gt;TakeawaysProblems&lt;/h2&gt;

&lt;h2 id=&quot;assumptions&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques&quot;&gt;Techniques&lt;/h2&gt;

&lt;h3 id=&quot;query-type&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;h2 id=&quot;workloads&quot;&gt;Workloads&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;h3 id=&quot;dynamic-workload&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;h1 id=&quot;1-dsb-datasets&quot;&gt;1. DSB datasets&lt;/h1&gt;

&lt;p&gt;Existing benchmarks lack of&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;more distinct query instances&lt;/li&gt;
  &lt;li&gt;dynamic workloads&lt;/li&gt;
  &lt;li&gt;guidance of comprehensively comparing the performance trade-offs for workload-driven and traditional database systems.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;shift&quot;&gt;Shift&lt;/h2&gt;

&lt;p&gt;Skew&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both exponential / Zipfian distributions are for those where the common value is more frequent&lt;/li&gt;
  &lt;li&gt;The exponential distribution is often used for modeling &lt;strong&gt;time-related data&lt;/strong&gt; or “waiting times,” while Zipfian is used to model scenarios where there is a &lt;strong&gt;ranking or ordering of items&lt;/strong&gt;, such as popularity.Zipfian&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;workloads-1&quot;&gt;Workloads&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;skews and correlations, skews on individual columns, correlations between columns in the same table, and correlations of columns across multiple tables.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;=&amp;gt; more skews and correlations to individual columns and multiple columns within a table and across tables.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;more join patterns: including non-equi joins and cyclic join graphs.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;=&amp;gt; new query templates to enrich the join patterns.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;slice data at a fine granularity with multiple predicate filters on table columns.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;=&amp;gt; fine-grained data slicing with additional predicate filters.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;=&amp;gt; configurable parameterization to generate queries.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;=&amp;gt; comprehensive user guidance.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h1 id=&quot;2-modeling-shifting-workloads-for-learned-database-systems&quot;&gt;2. Modeling Shifting Workloads for Learned Database Systems&lt;/h1&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;h2 id=&quot;problems&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;The key question is when and how best to re-train the model.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Retraining within near queries =&amp;gt; catastrophic forgetting, large queries =&amp;gt; time-consuming.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Workload imbalance leads to poor performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;assumptions-1&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;Assumes the &lt;strong&gt;underlying data&lt;/strong&gt; to be static.&lt;/p&gt;

&lt;h2 id=&quot;techniques-1&quot;&gt;Techniques&lt;/h2&gt;

&lt;h3 id=&quot;query-type-query-encoding&quot;&gt;Query Type, Query Encoding&lt;/h3&gt;

&lt;p&gt;As in other papers&lt;/p&gt;

&lt;h3 id=&quot;model-1&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;distributed shift detector (per l query) =&amp;gt; train a new model/retrain the existing model with the importance of past/current queries.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;KS test for measuring the distribution of each feature of two samples.&lt;/li&gt;
  &lt;li&gt;Bonferroni correction to adjust the significance level.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;online clustering algorithm (Bayesian non-parametric models) =&amp;gt; managing the replay buffer =&amp;gt; providing data to re-train.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cluster based on the query feature encoding, other than learned embedding or gradients.&lt;/li&gt;
  &lt;li&gt;propose DP-Medoides based K-Medoids, since it is more robust to outliers/supports customerize distnace computes.&lt;/li&gt;
  &lt;li&gt;manage the reply buffer such that its balance, diversified.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;re-training: combined loss of pass and current queries.&lt;/p&gt;

&lt;h2 id=&quot;workloads-2&quot;&gt;Workloads&lt;/h2&gt;

&lt;h3 id=&quot;datasets-1&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;IMDB with JOB&lt;/p&gt;

&lt;p&gt;DSB (50GB)&lt;/p&gt;

&lt;p&gt;the query join graph and/or the distribution of query &lt;strong&gt;predicates&lt;/strong&gt; may shift with time.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-1&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Generate a query pool with each query classes having 200 queries.&lt;/li&gt;
  &lt;li&gt;shifting workloads:
    &lt;ol&gt;
      &lt;li&gt;randomly pick three classes (fixed)&lt;/li&gt;
      &lt;li&gt;for each class, randomly pick 100 queries&lt;/li&gt;
      &lt;li&gt;concatenate those queries.&lt;/li&gt;
      &lt;li&gt;repeate from 2-4 to form a workload with queries A(100), B(100), C(100), A(100)…&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;measure performance after retraining and evaluate in other classes (except the current one).&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;3-balsa&quot;&gt;3. Balsa&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-1&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;Neo-impl is still not robust enough to generalize to unseen test queries and suffers from high variance.&lt;/p&gt;

&lt;p&gt;Balsa’s better generalization is due to a &lt;strong&gt;broader state coverage&lt;/strong&gt; offered by simulation, &lt;strong&gt;on-policy learning&lt;/strong&gt;, and safe exploration&lt;/p&gt;

&lt;p&gt;the main point of this paper is to learn without expert demonstrations.&lt;/p&gt;

&lt;h2 id=&quot;problems-1&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;Existing RL-based methods assume the availability of a mature query optimizer to learn from.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-2&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;the &lt;strong&gt;database content is kept static&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Updates to the schema, appends, or in-place updates can be handled by retraining.&lt;/p&gt;

&lt;p&gt;This assumption implies that the agent need not solve a learning problem with a shifting distribution. Another assumption is that Balsa currently optimizes select-project-join (SPJ) blocks.&lt;/p&gt;

&lt;h2 id=&quot;techniques-2&quot;&gt;Techniques&lt;/h2&gt;

&lt;h3 id=&quot;formulation&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;query optimization as an MDP:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;state: partial query plan.&lt;/li&gt;
  &lt;li&gt;action step in building a query plan in a &lt;strong&gt;bottom-up fashion&lt;/strong&gt;， either join two tables, or assign a scan method.&lt;/li&gt;
  &lt;li&gt;a reward is given only at the final (terminal) state, each action only gets zero rewards&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;query-type-1&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding-1&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;same as NEO&lt;/p&gt;

&lt;h3 id=&quot;process&quot;&gt;Process&lt;/h3&gt;

&lt;p&gt;learn to optimize queries without learning from an existing expert optimizer&lt;/p&gt;

&lt;p&gt;Simulation and reality phases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;simulation phase trains a value network based on the simple &lt;strong&gt;CE cost&lt;/strong&gt;, although this &lt;strong&gt;CE cost&lt;/strong&gt; is not accurate, it enables the agent to avoid the worst plans.&lt;/li&gt;
  &lt;li&gt;reality phase finetunes the value network.&lt;/li&gt;
  &lt;li&gt;during exploration, Balsa generates several best-predicted plans (instead of the best) and then picks the best unseen one out of them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Generalization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;train with high diversity, merge experiences with several independently agent with different seeds.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some techniques&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;timeouts,&lt;/li&gt;
  &lt;li&gt;exploration only on the best few plans.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;on-policy learning better than retraining.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;workloads-3&quot;&gt;Workloads&lt;/h2&gt;

&lt;p&gt;TPCH&lt;/p&gt;

&lt;h1 id=&quot;4-is-your-learned-query-optimizer-behaving-as-you-expect-a-machine-learning-perspective&quot;&gt;4. Is Your Learned Query Optimizer Behaving As You Expect? A Machine Learning Perspective&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-2&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;A number of join is an irrelevant proxy for execution time.&lt;/p&gt;

&lt;p&gt;GEQO is used for large join number queries, which can be disabled to fairly compare.&lt;/p&gt;

&lt;h2 id=&quot;problems-2&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;The absence of a &lt;strong&gt;unified reproducible framework&lt;/strong&gt; make it currently impossible to fairly compare the results of LQOs.&lt;/p&gt;

&lt;p&gt;Many filter combinations result in the same selectivity. Hence, the model would potentially suffer from the mismatch between features and target variables and will only perform well if this inconsistency is mitigated.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-3&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques-3&quot;&gt;Techniques&lt;/h2&gt;

&lt;h3 id=&quot;query-type--query-encoding&quot;&gt;Query Type &amp;amp; Query Encoding&lt;/h3&gt;

&lt;p&gt;All existing LQO are query-driven methods other than data-driven.&lt;/p&gt;

&lt;p&gt;Encoding schema for a query should be both &lt;strong&gt;expressive and robust&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;roubust: unique 1-to-1 mapping between the feature variables and the latency or cost of a query and its given plan&lt;/li&gt;
  &lt;li&gt;expressive: the encoding should clearly reflect both the global and local context.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bao does not use talbes but only cardinalities and costs.&lt;/p&gt;

&lt;p&gt;=&amp;gt; easier to retrain and schema-agnosticism.&lt;/p&gt;

&lt;p&gt;=&amp;gt; not one-to-one mapping: different filters in a query has same cardinalities for the same table. ??&lt;/p&gt;

&lt;p&gt;Bao deos not use query encoding =&amp;gt; increases the probability of converging to a local optimum.&lt;/p&gt;

&lt;h3 id=&quot;model-2&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;An architecture that chains the ML models like in HybridQO, where different target variables are served to different models in the ML pipeline&lt;/p&gt;

&lt;h2 id=&quot;workloads-4&quot;&gt;Workloads&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;JOB and STACK&lt;/p&gt;

&lt;p&gt;We do not use the STATS-CEB benchmark, as it was originally developed for &lt;strong&gt;challenges&lt;/strong&gt; in cardinality estimation as opposed to end-to-end query optimization, which is the focus of this paper.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-2&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;four worklods&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Leave One Out Sampling.&lt;/li&gt;
  &lt;li&gt;Random Sampling.&lt;/li&gt;
  &lt;li&gt;Base Query Sampling.&lt;/li&gt;
  &lt;li&gt;Covariate Shift on JOB&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;5-neo&quot;&gt;5. NEO&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-3&quot;&gt;Takeaways&lt;/h2&gt;

&lt;h2 id=&quot;problems-3&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;However, none demonstrate that their improved cardinality estimations actually lead to better query plans.&lt;/p&gt;

&lt;p&gt;Furthermore, unlike &lt;strong&gt;join order selection&lt;/strong&gt;, selecting &lt;strong&gt;join operators&lt;/strong&gt; (e.g., hash join, merge join) and choosing &lt;strong&gt;indexes&lt;/strong&gt; cannot be entirely reduced to cardinality estimation.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-4&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;existence of a Sample Workload consisting of queries representative of the user’s total workload&lt;/p&gt;

&lt;p&gt;existence of the underlying engine’s capabilities (i.e., exercising a representative set of operators).&lt;/p&gt;

&lt;h2 id=&quot;techniques-4&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;end-to-end learning approach, including join order, index, and physical operator selection.&lt;/p&gt;

&lt;h3 id=&quot;formulation-1&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;query optimization as an MDP:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;state: partial query plan.&lt;/li&gt;
  &lt;li&gt;action step in building a query plan in a &lt;strong&gt;bottom-up fashion&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;a reward is given only at the final (terminal) state, each action only gets zero rewards&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;query-type-2&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding-2&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;Query level information (&lt;strong&gt;join graph&lt;/strong&gt;) + plan level information (&lt;strong&gt;join order&lt;/strong&gt;).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;query-level:
    &lt;ul&gt;
      &lt;li&gt;treats the &lt;strong&gt;join graph as an adjacency matrix&lt;/strong&gt; and only encodes the &lt;strong&gt;upper triangular portion&lt;/strong&gt; into one dim vec.&lt;/li&gt;
      &lt;li&gt;column predicates vector: 1-hot, hist, row-vector. row vector is to represent each row as a sentence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;plan-level
    &lt;ul&gt;
      &lt;li&gt;plan as a tree, each node is a vector of size J+2R, J is a number of join types, and R is a number of relations. Each relation has two values: table use or not? scan type&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-3&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;Value Model (tree-convolution) to predict the final execution time for a &lt;strong&gt;partial or complete plan&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;value network does not predict the cost of a subplan, but rather the best possible latency achievable from an execution plan that includes a given subplan.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;best-first search + value network =&amp;gt; valuate iteration techniques to generate the final plan.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;init-state: every scan is unspecified and there are no joins.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;action: either (1) fusing together two roots with a join operator or (2) turning an unspecified scan into a table or index scan&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;not greedily follow the suggestions of the value network.&lt;/li&gt;
  &lt;li&gt;with bootstrapping (warm-ups), it can avoid sample inefficiency problem&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;retraining from scratch other than fine-tuning.&lt;/p&gt;

&lt;h2 id=&quot;workloads-5&quot;&gt;Workloads.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-2&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;JOB, TPCH, Corp&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-3&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;Train and test on various templates.&lt;/p&gt;

&lt;h1 id=&quot;6lero&quot;&gt;6.Lero&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-4&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;The hint set-based strategy has some intrinsic limitations&lt;/p&gt;

&lt;h2 id=&quot;problems-4&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;existing methods are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;unstable performance.&lt;/li&gt;
  &lt;li&gt;high learning costs.&lt;/li&gt;
  &lt;li&gt;slow model updating.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is unncessary to estimate cost with model.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-5&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;We assume &lt;strong&gt;constant resource budget per plan execution in this paper&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;techniques-5&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;Plan explorer + model two-class classification.&lt;/p&gt;

&lt;h3 id=&quot;formulation-2&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;pair-wise classification problem&lt;/p&gt;

&lt;h3 id=&quot;query-type-3&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding-3&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;each node in the tree: join method, scane method, cardinality, row wideth, table used. (No estimated costs)&lt;/p&gt;

&lt;h3 id=&quot;model-4&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;Pre-train on the synthetic workloads based on the &lt;strong&gt;estimated cost&lt;/strong&gt; in the postgresql.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;randomly generate a number of plans with different join orders and predicates on different tables,&lt;/li&gt;
  &lt;li&gt;randomly set the cardinality for each sub-plan of each plan and feed them into the native cost model (without executing the plans) to derive the estimated plan costs as labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Online training&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;get the real execution latency and re-train the model.&lt;/li&gt;
  &lt;li&gt;retrian every 100 queries on IMDB and STATS, every 30 queries on TPC-H and every 50 queries on TPC-DS.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;workloads-6&quot;&gt;Workloads.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-3&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;tpch, tpc-ds, stats, imdb.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-4&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;“time series split” strategy, always evaluated on the new template.&lt;/p&gt;

&lt;h1 id=&quot;7leon&quot;&gt;7.LEON&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-5&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;Theoretically, as long as the &lt;strong&gt;cardinality estimations and the cost model&lt;/strong&gt; are accurate, this architecture obtains the optimal query plan&lt;/p&gt;

&lt;p&gt;The db query optimizer can do transformation for logical expression, such as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;unnest an in/exists subquery to a semi-join query.&lt;/li&gt;
  &lt;li&gt;subquery pushing involves &lt;strong&gt;evaluating a subquery earlier&lt;/strong&gt; in the execution process and “pushing” its results into the main query. This changes the order of operations to reduce computational cost or improve query performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those rules cannot be used in ML-based method, and thus ML-basd method cannot conduct the plan enumeration completely.&lt;/p&gt;

&lt;p&gt;DB can be extendend with new operators, while the ML-based model can only learn in trail-and-error manner.&lt;/p&gt;

&lt;p&gt;cost model to initialize the model =&amp;gt; cold start problem.&lt;/p&gt;

&lt;h2 id=&quot;problems-5&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;ML-based methods learn domain-specifc knowledge in a data-driven manner. Thus, it suffers from the following two fundamental limitations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;only for spj query and cannot handle complicated query.&lt;/li&gt;
  &lt;li&gt;cold-start problem&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, ML should aid the traiditonal query optimizer otherthan replacing it. And ML model should start from the traditional query optimizer.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-6&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques-6&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;start from current performance and self-adjust to a certain dataset or workload.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;cost estimation as a contextual parise-ranking problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;robust plan exploration strategy&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;a plan with a higher ranking position should be explored with a higher probability&lt;/li&gt;
      &lt;li&gt;learned optimizer should correct its errors that lead to sub-optimal query plans&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;formulation-3&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;formalizes query optimization as a contextual pair-wise plan ranking problem&lt;/p&gt;

&lt;h3 id=&quot;query-type-4&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding-4&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;logical properties: output cardinality and join graph of the sub query.&lt;/p&gt;

&lt;p&gt;physical properties: plan tree and the physical property of the whole plan. Each node is one-hot encoding representing the physical operators and sort order.&lt;/p&gt;

&lt;h3 id=&quot;model-5&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;Intra-equivalent set Model: estimate the cost for each plan and uncertainty.&lt;/p&gt;

&lt;p&gt;Inter-equivalent set Model: it is used to prune the redundant search space.&lt;/p&gt;

&lt;h2 id=&quot;workloads-7&quot;&gt;Workloads.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-4&quot;&gt;Datasets&lt;/h3&gt;

&lt;h3 id=&quot;dynamic-workload-5&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;training efficiency and latency performance.&lt;/p&gt;

&lt;h1 id=&quot;8-base&quot;&gt;8. BASE&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-6&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;transfer the model pre-trained on the cost and then to the latency sounds promising.&lt;/p&gt;

&lt;p&gt;learn a mapping from cost to latency sounds promising.&lt;/p&gt;

&lt;h2 id=&quot;problems-6&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;The DBMS cost model can estimate the cost for an execution plan, which is expected to reflect the latency of the plan, but they are not positively correlated in practice&lt;/p&gt;

&lt;h2 id=&quot;assumptions-7&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques-7&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;Two stage RL-based framework.&lt;/p&gt;

&lt;p&gt;Stage 1: fit RL to chose the operation with leatest cost.&lt;/p&gt;

&lt;p&gt;Stage 2: transfer a &lt;strong&gt;pre-trained policy based on the cost&lt;/strong&gt; to a &lt;strong&gt;better policy&lt;/strong&gt; based on latency&lt;/p&gt;

&lt;p&gt;leverage cost as a trade-off to increase training efficiency and then transfer the pre-trained model based on the cost to a new model that can adapt to latency signals&lt;/p&gt;

&lt;h3 id=&quot;formulation-4&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;BASE formulates the procedure of locating and filling the gap between cost and latency as a variant of IRL.&lt;/p&gt;

&lt;h3 id=&quot;query-type-5&quot;&gt;Query Type&lt;/h3&gt;

&lt;p&gt;SPJ&lt;/p&gt;

&lt;h3 id=&quot;query-encoding-5&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;Same as NEO&lt;/p&gt;

&lt;h3 id=&quot;model-6&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;In the 2nd stage, we use a calibrated reward function by applying a parameterized calibration function to the cost signals.&lt;/p&gt;

&lt;p&gt;calibration function only makes the calibrated cost signals and the latency signals more correlated&lt;/p&gt;

&lt;h2 id=&quot;workloads-8&quot;&gt;Workloads.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-5&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;STATS, IMDB.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-6&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;None&lt;/p&gt;

&lt;h3 id=&quot;performance-1&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;Fast training&lt;/p&gt;

&lt;h1 id=&quot;9reinforcement-learning-with-tree-lstm-for-join-order-selection&quot;&gt;9.Reinforcement Learning with Tree-LSTM for Join Order Selection&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-7&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;This handle the schema change by using dynamic pooling to map anylengthh input to a fixed sized vector.&lt;/p&gt;

&lt;h2 id=&quot;problems-7&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;Existing work&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;fixed-length feature vectors cannot capture the structural information of a join tree, which may produce poor join plans.&lt;/li&gt;
  &lt;li&gt;Moreover, it may also cause retraining the neural network when handling schema changes (e.g., adding tables/columns) or multialias table names that are common in SQL queries.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;assumptions-8&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques-8&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;Two stage RL-based framework.&lt;/p&gt;

&lt;p&gt;Stage 1: pre-train the Deep Reinforcement Learning with cost,&lt;/p&gt;

&lt;p&gt;Stage 2: finetune the model with the latency.&lt;/p&gt;

&lt;h3 id=&quot;formulation-5&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;BASE formulates the procedure of locating and filling the gap between 푟휙and 푟(푙 )as a variant of IRL.&lt;/p&gt;

&lt;h3 id=&quot;query-type-6&quot;&gt;Query Type&lt;/h3&gt;

&lt;p&gt;SPJ&lt;/p&gt;

&lt;h3 id=&quot;query-encoding-6&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;query  + data&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;query: contains join information in a query.
i.e., matrix to represent all join relation, then flatten into one vector. Then it applies FC to get the final representation&lt;/li&gt;
  &lt;li&gt;columns:
    &lt;ul&gt;
      &lt;li&gt;numertical value: vector of 4 value, if column is in join, is =? is&amp;gt;? is&amp;lt;?&lt;/li&gt;
      &lt;li&gt;string value: only use the selectivity information and get the column representions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;table: concatenate all columns representions.&lt;/li&gt;
  &lt;li&gt;join tree: use tree-LSTM to learn a representation.
    &lt;ul&gt;
      &lt;li&gt;combine both Child-Sum Tree-LSTM and N-ar Tree LSTM.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-7&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;Representation: use a Tree-LSTM&lt;/p&gt;

&lt;p&gt;RL: use DQN.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cost training and latency tunning.&lt;/li&gt;
  &lt;li&gt;not move target from cost to latecy directly, but instead treate them as multi task learning, where train two Q network but use the same input representation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Handle the dynamic worklaod:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use dynamic pooling to encoding any dynamic length query&lt;/li&gt;
  &lt;li&gt;Adding columns also use dynamic pooling&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;workloads-9&quot;&gt;Workloads.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-6&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;tpch, imdb&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-7&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;insert/delete columns, add new tables etc.&lt;/p&gt;

&lt;h3 id=&quot;performance-2&quot;&gt;Performance&lt;/h3&gt;

&lt;h1 id=&quot;10theoretical-analysis-of-learned-database-operations-under-distribution-shift-through-distribution-learnability&quot;&gt;10.Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-8&quot;&gt;Takeaways&lt;/h2&gt;

&lt;h2 id=&quot;problems-8&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;no theoretical work shows any advantage in using the learned methods in dynamic datasets and under distribution shift.&lt;/p&gt;

&lt;p&gt;The goal of this paper is to theoretically understand the capabilities of learned models for database operations, show why and when they outperform non-learned alternatives and provide theoretical guarantees on their performance&lt;/p&gt;

&lt;h2 id=&quot;assumptions-9&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques-9&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;indexing, cardinality estimation and sorting when presence of insertions from a possibly changing data distribution&lt;/p&gt;

&lt;h3 id=&quot;formulation-6&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;data after inserting n times is D_n&lt;/p&gt;

&lt;p&gt;its distribution is learnable with parameter T, S, B.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;T: the model can be evaluted in T operatios.&lt;/li&gt;
  &lt;li&gt;take S to store.&lt;/li&gt;
  &lt;li&gt;n*B to learn the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;11eraser-eliminating-performance-regression-on-learned-query-optimizer&quot;&gt;11.Eraser: Eliminating Performance Regression on Learned Query Optimizer&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-9&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;This is mainly focus on the dataset side other than model side, for any trained model, it filter the search spcae based on model performance to increate the effectiveness of the model.&lt;/p&gt;

&lt;h2 id=&quot;problems-9&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;learned model has regression due to reasons like&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;inherent difficulty of the learning problem,&lt;/li&gt;
  &lt;li&gt;the low generalization ability of the prediction model on new data,&lt;/li&gt;
  &lt;li&gt;the under-fitting on training data due to insufficient training data,&lt;/li&gt;
  &lt;li&gt;loss of features, noisy labels and inappropriate model training methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;assumptions-10&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;Learned model has low generalizatin ability, thus has big regression problem.&lt;/p&gt;

&lt;h2 id=&quot;techniques-10&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;Two stage RL-based framework.&lt;/p&gt;

&lt;p&gt;Stage 1: coarse-grained filter to remove highly risky plans&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insight: ML prediction model is likely to underperform due to encountering &lt;strong&gt;unseen feature values&lt;/strong&gt;, therefore &lt;strong&gt;unexpected plan space&lt;/strong&gt; into &lt;strong&gt;subspaces&lt;/strong&gt; based on unseen feature values. this is like data labelling process.&lt;/li&gt;
  &lt;li&gt;train a model to predict &lt;strong&gt;precise&lt;/strong&gt; or &lt;strong&gt;imprecise&lt;/strong&gt; of each subspace.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stage 2: cluster plans in fine-grained manner, evalutes each cluster based on the prediction result.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cluster the remaining precise plans into groups based on their feature values and the model’s reliability.&lt;/li&gt;
  &lt;li&gt;Assign each cluster a &lt;strong&gt;reliability interval&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;formulation-7&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;A performance elimination method perf_elim such that benefits are maximum and loss is minimized.&lt;/p&gt;

&lt;h3 id=&quot;query-type-7&quot;&gt;Query Type&lt;/h3&gt;

&lt;p&gt;SPJ&lt;/p&gt;

&lt;h3 id=&quot;query-encoding-7&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;plan-level features: join relations, filtering predicates and operator types&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;join: join type, scan type,&lt;/li&gt;
  &lt;li&gt;filter: low and high value&lt;/li&gt;
  &lt;li&gt;relation: 0/1 to indicate existance.&lt;/li&gt;
  &lt;li&gt;structure: use a categorical variable to indicates it, bushy tree, left-deep and right-deep, on plans joining 4 tables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;data-level features: estimated cardinality and data distribution&lt;/p&gt;

&lt;h3 id=&quot;model-8&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;unexpected plan explorer filter bad plans based on the &lt;strong&gt;relability threshold.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;cluster the remining plans and use the &lt;strong&gt;segment model&lt;/strong&gt;, and also assign &lt;strong&gt;relability value&lt;/strong&gt; to reflect the quality.&lt;/p&gt;

&lt;p&gt;Then filter again, and use &lt;strong&gt;plan selection method&lt;/strong&gt; to balance the benefit of &lt;strong&gt;improvements&lt;/strong&gt; and the &lt;strong&gt;risk of regression&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;workloads-10&quot;&gt;Workloads.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-7&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;tpch, imdb&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-8&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;trian on 25% etc and evaluate on the fixed test dataset.&lt;/p&gt;

&lt;h3 id=&quot;performance-3&quot;&gt;Performance&lt;/h3&gt;

&lt;h1 id=&quot;12access-path-selection-in-a-relational-database-management-system&quot;&gt;12.Access Path Selection in a Relational Database Management System&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-10&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;This handle the schema change by using dynamic pooling to map anylengthh input to a fixed sized vector.&lt;/p&gt;

&lt;h2 id=&quot;problems-10&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;This paper will address the issues of access path selection for queries&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;single table queries&lt;/li&gt;
  &lt;li&gt;join&lt;/li&gt;
  &lt;li&gt;nested queries&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;techniques-11&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;query processing are 4 phases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;parsing:&lt;/li&gt;
  &lt;li&gt;optimization:&lt;/li&gt;
  &lt;li&gt;code generation:&lt;/li&gt;
  &lt;li&gt;execution:&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;13cardbench-a-benchmark-for-learned-cardinality-estimation-in-relational-databases&quot;&gt;13.CardBench: A Benchmark for Learned Cardinality Estimation in Relational Databases&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-11&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;learn from the transferable features, and not from the workload specific features such as table and attribute names.This approach enables learning of zero-shot CE models.&lt;/p&gt;

&lt;h2 id=&quot;problems-11&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;Traditional CE techniques used in modern database systems have well-known limitations, as they &lt;strong&gt;make simplistic data modeling assumptions, such as data uniformity and independence of columns in the tables&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Learned CE have not been adapted in practice, due to their high training overheads.&lt;/p&gt;

&lt;h2 id=&quot;techniques-12&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;zero-shot models are pre-trained on a &lt;strong&gt;broad spectrum of different datasets&lt;/strong&gt; and use &lt;strong&gt;transferable features (dataset-agnostic)&lt;/strong&gt;, such as table sizes, allowing them to generalize to unseen datasets.&lt;/p&gt;

&lt;h3 id=&quot;query-type-8&quot;&gt;Query Type&lt;/h3&gt;

&lt;p&gt;SPJ&lt;/p&gt;

&lt;h3 id=&quot;model-9&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;GNN model to encode the query structure.&lt;/p&gt;

&lt;p&gt;MLP-based cardinality model to get the final prediction.&lt;/p&gt;

&lt;h2 id=&quot;workloads-11&quot;&gt;Workloads.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-8&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;tpch, imdb&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-9&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;h3 id=&quot;performance-4&quot;&gt;Performance&lt;/h3&gt;

&lt;h1 id=&quot;14loger-a-learned-optimizer-towards-generating-efficient-and-robust-query-execution-plans&quot;&gt;14.LOGER: A Learned Optimizer towards Generating Efficient and Robust Query Execution Plans&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-12&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;This work aims to change the search space compared with bao, but still use hint, it input action(disable a join algorithm) and input join order into the network to predict the latency.&lt;/p&gt;

&lt;h2 id=&quot;problems-12&quot;&gt;Problems&lt;/h2&gt;

&lt;h2 id=&quot;assumptions-11&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques-13&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;reward weighting&lt;/strong&gt; to reduce the &lt;strong&gt;impact of previous poor operators&lt;/strong&gt; and make LOGER pay less attention to disastrous plans by log transformation.&lt;/p&gt;

&lt;h3 id=&quot;formulation-8&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;RL to build a tree&lt;/p&gt;

&lt;p&gt;state: subplan’s join order and operator&lt;/p&gt;

&lt;p&gt;action: join table and with restricted operator (disable a join algorithm, like disable hash join)&lt;/p&gt;

&lt;h3 id=&quot;query-type-9&quot;&gt;Query Type&lt;/h3&gt;

&lt;p&gt;spj without sub-queries&lt;/p&gt;

&lt;h3 id=&quot;query-encoding-8&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;Graph Transformer: GT not only efficiently captures relationships between table representations that include both table and predicate information, but also integrates global structural information of join graph into table representations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;table: table-level learned embedding vector.&lt;/li&gt;
  &lt;li&gt;column: column-level statistic vector with column type, proportion of unique values, unique value count, null values, index.&lt;/li&gt;
  &lt;li&gt;predicts: join exist or not, selectivity (via approximation of DBMS cardinality estimator.)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-10&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;The value model &lt;strong&gt;evaluates state-action pair&lt;/strong&gt; candidates of subplans by predicting the reachable lowest latency among all plans that can be reached from the pair.&lt;/p&gt;

&lt;p&gt;state network:&lt;/p&gt;

&lt;p&gt;action network: take all join as input,&lt;/p&gt;

&lt;h2 id=&quot;workloads-12&quot;&gt;&lt;strong&gt;Workloads&lt;/strong&gt;.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-9&quot;&gt;Datasets&lt;/h3&gt;

&lt;h3 id=&quot;dynamic-workload-10&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;change schema.&lt;/p&gt;

&lt;h3 id=&quot;performance-5&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;2 hours of training, 2x speedup&lt;/p&gt;

&lt;h1 id=&quot;15prestos-history-based-query-optimizer&quot;&gt;15.Presto’s History-based Query Optimizer&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-13&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;share-everything =&amp;gt; latency over scalability.&lt;/p&gt;

&lt;h2 id=&quot;problems-13&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;learning based CE/cost method requires large trainng efforts, less robustness, and hard to debug.&lt;/p&gt;

&lt;h2 id=&quot;techniques-14&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;HBO tracks q&lt;strong&gt;uery execution statistics&lt;/strong&gt; at the operator node, and &lt;strong&gt;uses those to predict future performance&lt;/strong&gt; for &lt;strong&gt;similar&lt;/strong&gt; queries.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Similary query fetching: only record the template,&lt;/li&gt;
  &lt;li&gt;HBO is more powerful than CBO as it can store various runtime statistics related to scheduling as well&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;join reordering: HBO record the stats for some join order, and thus accurate. If not meet before, we use the CBO.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;16parqo-penalty-aware-robust-plan-selection-in-query-optimization&quot;&gt;16.PARQO: Penalty-Aware Robust Plan Selection in Query Optimization&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-14&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;This paper mentioned that some CE is sensitive while other are not.&lt;/p&gt;

&lt;p&gt;The whole paper based on the probability of f(s∣s′):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It starts as an error distribution estimated from historical data.&lt;/li&gt;
  &lt;li&gt;It drives sensitivity analysis by identifying critical dimensions.&lt;/li&gt;
  &lt;li&gt;It enables robust plan selection by sampling likely true selectivities and guiding the optimizer to focus on realistic scenarios.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problems-14&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;Accurate estimation of the cardinality and cost comes at the cost of runtime monitoring and ongoing maintenance.&lt;/p&gt;

&lt;p&gt;Therefore, how much influence of such uncertainty bring to the selectivity estimates still underexplored.&lt;/p&gt;

&lt;p&gt;Robustness is tied to the uncertainty, and use the uncertainty in practise has challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;errors distributed on data and query worklaod is not well utiized in exsting work.&lt;/li&gt;
  &lt;li&gt;complex query has large cardinality estimate spaces, and thus has overhead to evaluate all other competing plans.&lt;/li&gt;
  &lt;li&gt;support learned optimzier at scale and in efficient way.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;assumptions-12&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques-15&quot;&gt;Techniques&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;getting the distribution of f(s&lt;/td&gt;
      &lt;td&gt;s`), where s` is estimated cardinalities, and s is the true selectivities.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;based on the distribution, find the &lt;strong&gt;sensitive&lt;/strong&gt; dimensions for a given query plan got from estimates s`.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;identify the dimensions with biggest impact on the user-defined penalty function.&lt;/li&gt;
  &lt;li&gt;using sobol method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;build a pool of candidate plan by sampling from the distribution of true selectivities conditioned on their estimates, then select one with lowest penalty.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Error profling:&lt;/p&gt;

    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;build a model to estimate  f(s&lt;/td&gt;
          &lt;td&gt;s`) given Q and s`&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

    &lt;p&gt;break the query into querylet, which is a subquery pattern. like join with local selection.&lt;/p&gt;

    &lt;p&gt;for each querlet in (single-table querylet, two table querley, three-table querylet with fixed pattern), measure the true cardinalities and estimated cardinalities.&lt;/p&gt;

    &lt;p&gt;one model for low-selectivity, one model for hight selectivity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sensitive analysis&lt;/p&gt;

    &lt;p&gt;it uses &lt;strong&gt;sobol sensitiity&lt;/strong&gt; to identify the sensitive selectivity dimensions for a given plan.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Plan selection
use sampling to sample a pool of plans based on the probability f(s, s`),
estimate the cost&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;formulation-9&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;No&lt;/p&gt;

&lt;h3 id=&quot;query-type-10&quot;&gt;Query Type&lt;/h3&gt;

&lt;p&gt;No&lt;/p&gt;

&lt;h3 id=&quot;query-encoding-9&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;No&lt;/p&gt;

&lt;h3 id=&quot;model-11&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;KDE&lt;/p&gt;

&lt;h2 id=&quot;workloads-13&quot;&gt;&lt;strong&gt;Workloads&lt;/strong&gt;.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-10&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;IMDB, DSB, STATS&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-11&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;split database into multiple instance in time series manner.&lt;/p&gt;

&lt;h3 id=&quot;performance-6&quot;&gt;Performance&lt;/h3&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;ok, so from the beginning of the execution, i think this is the overall logic&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;it pre-defines some query templates, and then run lots of queries and collect 
(target dimension (which is only base table + number of join between any two pairs), estimate seleivity, and real seleivity.)&lt;/li&gt;
  &lt;li&gt;given a query, it will first fit a kde model for the error of each target dimension based on this datasets, and get a list of kde for each target dimension, then it sample 1000 points and get mean selectivity error value as the so called real error.&lt;/li&gt;
  &lt;li&gt;it then perform sensitive analysis. it first get the all predefined possible sensitive dims, named as pre_defined_querylet_dims,which may not exactly the same the target dimensions. (from the paper the thoery, if the dimensions in the target dimention has no selection, then we don;t care it, it’s CE in pg is accurate)&lt;/li&gt;
  &lt;li&gt;It get optimizaion plan fisrt (this is by feeding the optimzier with the real cardinalities and selectivility), 
then it get the hint to generate this plan in later. 
in the adjust_selectivity process, it first update selective of the the target_dimensions and update by provided error_samples, which can be inaccurate, since it is to measure the influence, is it?
And then it updates all others dimensiton which is not in the target_dimensions but in the sampled_real_sel_err_mean_list, 
each key in the sampled_real_sel_err_mean_list is actually the predefined possible sensitives, we make all those who are not in the target as accurate as possible, so it will not casuing any influence in later. 
therefore, those dimens are update based on the sampled_real_sel_err_mean_list, while the target is based on the error_samples.
finally all changes are updated to the updated_join_sel, which is a litst of original q_card_info.est_join_sel and note, this incldue all the join imforamtions, not one-to-one join, but one-to-multiple joins.
here is a questions, the code after “what the fk is this” feels like has not effetc, is it?&lt;/li&gt;
  &lt;li&gt;it also get the default execution plan, which is the postgresql plan&lt;/li&gt;
  &lt;li&gt;then it generate sampels from joint error dist , name is joint error dist, but actually it sampled indenpendently. gfinally it get A and B sampels.&lt;/li&gt;
  &lt;li&gt;then it generates a problem, which is basiclaly for the sobol.&lt;/li&gt;
  &lt;li&gt;then i think i’m not familiart with the implementatin of the sobol, what is tht following do? in the theory, i think i just need to estimate ethe improtance of reach dimension by computing s = var()/Var(Y), is it? and we use MC sampling to do that, so we need A, B , and mc smapling also change the computing method of s. i only know htose&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;17plaque-automated-predicate-learning-at-query-time&quot;&gt;17.PLAQUE: Automated Predicate Learning at Query Time&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-15&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;Discovering the proper predicates is also important to improve the efficiency.&lt;/p&gt;

&lt;h2 id=&quot;problems-15&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;Predicates pushdown is useful in speeding up the query processing, but query optimzier only pushdown the predictes exist in the query. However, tables with no predicated also has a chance to prune.&lt;/p&gt;

&lt;p&gt;Therefore, this paper try to discover the predicates and add that to the query in query execution.&lt;/p&gt;

&lt;h1 id=&quot;18-fastgres-making-learned-query-optimizer-hinting-effective&quot;&gt;18. FASTgres: Making Learned Query Optimizer Hinting Effective&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-16&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;This paper learn multiple GB models for each context, reduing the model complexity and training inefficiency. But not quite handle the change of data or query ? data is not changed in this setting.&lt;/p&gt;

&lt;h2 id=&quot;problems-16&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;bao cannot achieve the potetial to the full extent.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-13&quot;&gt;Assumptions&lt;/h2&gt;

&lt;h2 id=&quot;techniques-16&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;Divide and conquer approach.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;workload -&amp;gt; partitioned into query groups, each with different join tables and join predicates -&amp;gt; context.&lt;/li&gt;
  &lt;li&gt;learn a separate and context-sensitive ML model for each context.
here since the partition is based on join pattern, join is not required to learn, we can only learn to map the filter predicates to the hint set.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;formulation-10&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;Divide and conquer approach.&lt;/p&gt;

&lt;h3 id=&quot;query-type-11&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding-10&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;predicate-based encoding&lt;/p&gt;

&lt;h3 id=&quot;model-12&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;GB models&lt;/p&gt;

&lt;h2 id=&quot;workloads-14&quot;&gt;&lt;strong&gt;Workloads&lt;/strong&gt;.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-11&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;Stack [18], JOB [13], and TPC-H&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-12&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;randonmly&lt;/strong&gt; sample training query&lt;/p&gt;

&lt;h3 id=&quot;performance-7&quot;&gt;Performance&lt;/h3&gt;

&lt;h1 id=&quot;19os-pre-trained-transformer-predicting-query-latencies-across-changing-system-contexts&quot;&gt;19.OS Pre-trained Transformer: Predicting Query Latencies across Changing System Contexts&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-17&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;This paper propose to train a generatl embedding to ecndoe the informatin of the system envs, such as cpu, memory etc, and finally combine the result of this and instance-specifc embedding together to predict the final result of the task.&lt;/p&gt;

&lt;h2 id=&quot;problems-17&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;Model leared in one context fail when tested on a new system context. Thus, there is a generalization gap.&lt;/p&gt;

&lt;p&gt;Therefore, existing work cannot utilize the interaction of different resources to accurately predict query performance.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-14&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;system state does not change drastically just before and during query execution.&lt;/p&gt;

&lt;h2 id=&quot;techniques-17&quot;&gt;Techniques&lt;/h2&gt;

&lt;h3 id=&quot;formulation-11&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;query plan, OS logs -&amp;gt; query latency.&lt;/p&gt;

&lt;h3 id=&quot;query-type-12&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding-11&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;GCN for query plans, the model should produce an embedding vector with all query specifc properties to predict the latency.&lt;/p&gt;

&lt;h3 id=&quot;model-13&quot;&gt;Model&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Database specifialized query plan model: encoding the plan into a vecotr. like using &lt;strong&gt;GCN&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;A universal (i.e., applicable to any workload) transformer model&lt;/strong&gt; which takes as input recent OS logs, and produces an embedding vector that captures the system state.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Embedding vectors are used as input to a universal Multi Layer Perceptron (MLP) module to produce the latency prediction.&lt;/li&gt;
  &lt;li&gt;When in new datasets, it fix the Osprey model and prediction head, and only update the query embedding model, the GCN.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;workloads-15&quot;&gt;&lt;strong&gt;Workloads&lt;/strong&gt;.&lt;/h2&gt;

&lt;h3 id=&quot;datasets-12&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;150k PostgreSQL query execution plans and corresponding linux system logs for over 2500 unique queries from 14 database workloads executed on 10 AWS instance types, and under various system loads&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-13&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;h3 id=&quot;performance-8&quot;&gt;Performance&lt;/h3&gt;

&lt;h1 id=&quot;20-zero-shot-cost-models-for-out-of-the-box-learned-cost-prediction&quot;&gt;20. Zero-Shot Cost Models for Out-of-the-box Learned Cost Prediction&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-18&quot;&gt;Takeaways&lt;/h2&gt;

&lt;h2 id=&quot;problems-18&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;training data collection is high cost.&lt;/p&gt;

&lt;p&gt;simply model often under/over-estimates the true costs of queries, since they cannot capture complex interactions in the query plan and data.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-15&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;we only focus on the transfer of &lt;strong&gt;learned cost models&lt;/strong&gt; across databases for a &lt;strong&gt;single database system on a fixed hardware.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;techniques-18&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;New query and data representation that allows zero-shot cost models to be pre-trained across databases.&lt;/p&gt;

&lt;h3 id=&quot;formulation-12&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;The goal of &lt;strong&gt;zero-shot cost estimation&lt;/strong&gt; is to predict query latencies (i.e., runtimes) on an &lt;strong&gt;unseen database&lt;/strong&gt; without having observed any query on this &lt;strong&gt;unseen database&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;query-type-13&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding-12&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;p&gt;A new representation of &lt;strong&gt;queries&lt;/strong&gt; that can generalize across databases.&lt;/p&gt;

&lt;p&gt;Encoded information:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;physical plan operators as nodes.&lt;/li&gt;
  &lt;li&gt;input columns, tables, predicate information.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Encoded in transferable way:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;predicate structure: data types of columns/operators, intermediate cardinalities&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-14&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;a database-agnostic model to &lt;strong&gt;estimate the runtime cost&lt;/strong&gt; and a databasedependent model (e.g., histograms) to capture data characteristics.&lt;/p&gt;

&lt;p&gt;database-agnostic cost model: feed with both estimated cardinalities and output of database-dependent model, it generates a cost.&lt;/p&gt;

&lt;h2 id=&quot;workloads-16&quot;&gt;&lt;strong&gt;Workloads&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;datasets-13&quot;&gt;Datasets&lt;/h3&gt;

&lt;h3 id=&quot;dynamic-workload-14&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;h3 id=&quot;performance-9&quot;&gt;Performance&lt;/h3&gt;

&lt;h1 id=&quot;21-rethinking-learned-cost-models-why-start-from-scratch&quot;&gt;21. Rethinking Learned Cost Models: Why Start from Scratch?&lt;/h1&gt;

&lt;h2 id=&quot;takeaways-19&quot;&gt;Takeaways&lt;/h2&gt;

&lt;h2 id=&quot;problems-19&quot;&gt;Problems&lt;/h2&gt;

&lt;p&gt;end to end learning based cost model is hard to interpret and use, and retraining is costly.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;learning-based models are trained on a specific database with specific hardware and software configurations. retraining is costly.&lt;/li&gt;
  &lt;li&gt;data collection is time-consuming and expensive process.&lt;/li&gt;
  &lt;li&gt;no insight of how the cost estimate was generated compared with the rules-based cost model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;learned cost model should be easy to train, hight transferability, and interpretabe.&lt;/p&gt;

&lt;h2 id=&quot;assumptions-16&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;we assume that the formula-based cost model can provide a good estimation, if all R-params in Table 1 are set correctly.&lt;/p&gt;

&lt;h2 id=&quot;techniques-19&quot;&gt;Techniques&lt;/h2&gt;

&lt;p&gt;Two stage offlien training stage and online refinement stage.&lt;/p&gt;

&lt;p&gt;Static and Dynamic C parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hardware, OS, database can be &lt;strong&gt;static configuration parameters.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;data type and column correlation are dynamic configuration parameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Data collection:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Execute queries on different CPU, memory, SSDs, HDDs, operating systems, etc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Offline phase: capature relations between hyperparamers and static configuration parameters.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;pre-train a decision tree for each of 8 operator, there are totally 8 trees.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Online phase: adaptively tune each tree for the dynamic query/data factors.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hundreds of dynamic configuration parameters, such as query related, data related and 97 configuration parameters.&lt;/li&gt;
  &lt;li&gt;online phase invoked when the cost model cannot provide a precise estimation for more than K queries.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;few-show recommendatin model&lt;/strong&gt; to rank all candidate parameters based on the effects on cost estimations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;formulation-13&quot;&gt;Formulation&lt;/h3&gt;

&lt;p&gt;based on the system and workload configutaion, it learn a mapping from the those configutaion to the hyperparameter of the cost function, i.e., determine optimal choices of R-params in the built-in cost formulas of major database.&lt;/p&gt;

&lt;p&gt;G: C -&amp;gt; R&lt;/p&gt;

&lt;p&gt;The total operators considered are 8.&lt;/p&gt;

&lt;h3 id=&quot;query-type-14&quot;&gt;Query Type&lt;/h3&gt;

&lt;h3 id=&quot;query-encoding-13&quot;&gt;Query Encoding&lt;/h3&gt;

&lt;h3 id=&quot;model-15&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;decision tree model.&lt;/p&gt;

&lt;h2 id=&quot;workloads-17&quot;&gt;&lt;strong&gt;Workloads&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;datasets-14&quot;&gt;Datasets&lt;/h3&gt;

&lt;p&gt;IMDB, TPC-H and TPC-DS&lt;/p&gt;

&lt;h3 id=&quot;dynamic-workload-15&quot;&gt;Dynamic Workload&lt;/h3&gt;

&lt;h3 id=&quot;performance-10&quot;&gt;Performance&lt;/h3&gt;

&lt;p&gt;###&lt;/p&gt;
</description>
        <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/LQO/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/LQO/</guid>
        
        
      </item>
    
      <item>
        <title>Distribution Shifts</title>
        <description>&lt;p&gt;&lt;strong&gt;Covariate Shift&lt;/strong&gt;: P(X) changes, but P(Y∣X) stays the same.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Label Shift&lt;/strong&gt;: P(Y) changes, but P(X∣Y) stays the same.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Concept Shift&lt;/strong&gt;: P(Y∣X) changes.&lt;/p&gt;

&lt;p&gt;I’m currently researching, where i need to use machine learning to enhance query optimizer.&lt;/p&gt;

&lt;p&gt;My current plan is three steps&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;I learned from the paper “Bao: Learning to Steer Query Optimizers”, which uses a set of hints to generate a set of query plans, which I follow. From a Refincemenrt learning perspective, each query plan here is one action,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I then try to use a new model to predict which one is better. so I need to first feature the input and the model can predict which plan is better.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;if we use RL to model this, each hint in the hint set is one action, the state can be the input SQL, cpu/memory etc,. the reward can be the 1/latency.&lt;/p&gt;

    &lt;p&gt;While in this situation, we simply can train the decision transformer, however, i basically do not use the query plan structure, which is generated by the database optimizer via the hint, is it?&lt;/p&gt;

    &lt;p&gt;As for the query plan,&lt;/p&gt;

    &lt;p&gt;i firstly use a paper’s method (queryFormer)  to convert a query plan tree into a fixed vector encoding of dimension (batch_size=1024, dimension=329).&lt;/p&gt;

    &lt;p&gt;Then I wanna use the decision transformer to predict which one is the best action.&lt;/p&gt;

    &lt;p&gt;while predicting the best hint is bascially find the query palan which has the mini exeucution latency,&lt;/p&gt;

    &lt;p&gt;but not using anything from the query plan is a bad idea.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;how to use the query plan into my current setting? or ydo you have gbetter idea to add those information?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/219/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/219/</guid>
        
        
        <category>A paper note</category>
        
      </item>
    
      <item>
        <title>Supervised Pretraining Can Learn In-Context Reinforcement Learning</title>
        <description>
</description>
        <pubDate>Thu, 30 May 2024 00:00:00 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/218/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/218/</guid>
        
        
        <category>A paper note</category>
        
      </item>
    
      <item>
        <title>IN-CONTEXT EXPLORATION-EXPLOITATION FOR REINFORCEMENT LEARNING</title>
        <description>
</description>
        <pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/216/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/216/</guid>
        
        
        <category>A paper note</category>
        
      </item>
    
      <item>
        <title>TABPFN A TRANSFORMER THAT SOLVES SMALL TABULAR CLASSIFICATION PROBLEMS IN A SECOND</title>
        <description>
</description>
        <pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/215/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/215/</guid>
        
        
        <category>A paper note</category>
        
      </item>
    
      <item>
        <title>TRANSFORMERS CAN DO BAYESIAN INFERENCE</title>
        <description>&lt;h3 id=&quot;enhanced-summary&quot;&gt;Enhanced Summary&lt;/h3&gt;

&lt;p&gt;This paper introduces Prior-Data Fitted Networks (PFNs), a novel approach leveraging Transformers to approximate Bayesian inference. PFNs aim to overcome challenges associated with deep learning for Bayesian methods, such as explicit prior specification and accurate uncertainty capture. By transforming posterior approximation into a supervised classification problem, PFNs can make probabilistic predictions with a single forward pass, efficiently mimicking Gaussian Processes (GPs) and enabling Bayesian inference for intractable problems with significant speedups.&lt;/p&gt;

&lt;h3 id=&quot;gaussian-processes-and-bayesian-inference&quot;&gt;Gaussian Processes and Bayesian Inference&lt;/h3&gt;

&lt;p&gt;Gaussian Processes (GPs) are a powerful tool in Bayesian inference, providing a non-parametric way to model distributions over functions. Bayesian inference involves updating prior beliefs based on observed data to make predictions. GPs facilitate this by defining prior over functions and using observed data to compute the posterior distribution, which can then be used for predictions.&lt;/p&gt;

&lt;h3 id=&quot;relationship-between-gps-and-the-paper&quot;&gt;Relationship Between GPs and the Paper&lt;/h3&gt;

&lt;p&gt;The paper demonstrates that PFNs can effectively approximate the posterior predictive distribution (PPD) of GPs. This is significant because GPs are known for their ability to provide well-calibrated uncertainty estimates and handle small datasets effectively. By approximating GPs, PFNs inherit these desirable properties, making them a versatile tool for Bayesian inference across various tasks.&lt;/p&gt;

&lt;h3 id=&quot;variational-inference-vi-and-markov-chain-monte-carlo-mcmc&quot;&gt;Variational Inference (VI) and Markov Chain Monte Carlo (MCMC)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Variational Inference (VI):&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;VI approximates the posterior distribution by finding a tractable distribution that is close to the true posterior. This is done by optimizing the parameters of the approximate distribution to minimize the Kullback-Leibler (KL) divergence from the true posterior.&lt;/li&gt;
      &lt;li&gt;The paper compares PFNs with stochastic variational inference (SVI), a specific VI method, highlighting the efficiency and accuracy improvements achieved by PFNs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Markov Chain Monte Carlo (MCMC):&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;MCMC methods, such as the No-U-Turn Sampler (NUTS), generate samples from the posterior distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution.&lt;/li&gt;
      &lt;li&gt;PFNs are shown to significantly outperform MCMC methods in terms of speed, achieving up to 8,000 times faster inference while maintaining comparable accuracy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proposed-solution-and-architectural-details&quot;&gt;Proposed Solution and Architectural Details&lt;/h3&gt;

&lt;p&gt;The paper proposes using a Transformer-based architecture without positional encodings to maintain permutation invariance in the input dataset. Key details include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Transformer Architecture:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The model is a Transformer encoder with no positional encodings, ensuring invariance to the order of the dataset ( D ).&lt;/li&gt;
      &lt;li&gt;Inputs and queries are fed as linear projections to the Transformer, which then outputs the PPD for each query based on the dataset and query.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mathematical Formulation:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;The loss function used for training PFNs is the Prior-Data Negative Log-Likelihood (Prior-Data NLL):
[
\ell_\theta = \mathbb{E}&lt;em&gt;{D \cup {x,y} \sim p(D)} [-\log q&lt;/em&gt;\theta(y|x,D)]
]&lt;/li&gt;
      &lt;li&gt;This objective ensures that minimizing the loss yields an approximation of the PPD in terms of cross-entropy and KL-Divergence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Training Process:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;PFNs are trained by sampling datasets from a prior distribution and fitting the model to predict hold-out examples.&lt;/li&gt;
      &lt;li&gt;The model is optimized using stochastic gradient descent on the Prior-Data NLL.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Experiment Details and Performance:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;PFNs were evaluated on tasks such as GP regression, Bayesian neural networks, classification for small tabular datasets, and few-shot image classification.&lt;/li&gt;
      &lt;li&gt;PFNs achieved over 200-fold speedups compared to traditional methods, with significant performance improvements demonstrated in various experiments.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;implementation-steps&quot;&gt;Implementation Steps&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Define Prior Distribution:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Sample datasets from a prior distribution ( p(D) ).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Train the PFN:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Initialize the Transformer model.&lt;/li&gt;
      &lt;li&gt;Train the model by minimizing the Prior-Data NLL using stochastic gradient descent.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Perform Inference:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;For a given dataset ( D ) and query ( x ), use the trained PFN to predict the PPD for ( x ).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Performance Improvement:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;PFNs closely approximate the PPD of GPs, with results nearly indistinguishable from the exact PPD.&lt;/li&gt;
      &lt;li&gt;Significant speedups were observed, with PFNs being 1,000 times faster than Bayes-by-Backprop SVI and up to 8,000 times faster than NUTS.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Implementation Steps:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Train PFNs using large-scale datasets generated from priors.&lt;/li&gt;
      &lt;li&gt;Fine-tune the model for specific tasks as needed, demonstrating flexibility and efficiency in practical applications.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;PFNs represent a significant advancement in leveraging deep learning techniques for Bayesian inference, offering a scalable and efficient alternative to traditional methods. With the ability to approximate GPs and handle diverse tasks, PFNs provide a versatile tool for Bayesian inference, achieving remarkable speed and performance improvements.&lt;/p&gt;
</description>
        <pubDate>Wed, 15 May 2024 00:00:00 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/214/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/214/</guid>
        
        
        <category>A paper note</category>
        
      </item>
    
      <item>
        <title>ALECE An Attention-based Learned Cardinality Estimator for SPJ Queries on Dynamic Workloads (Extended)</title>
        <description>&lt;p&gt;Query-driven and data-driven methods&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;query-driven: it basically trains a model to predict CE.&lt;/li&gt;
  &lt;li&gt;data-driven: it basically learns a join distribution among columns and then sampling based on that, and use the sampled data to estimate the basic statistics. The default optimizer will then use those statistics to estimate CE.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Existing work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cannot combine both query-driven and data-driven methods.&lt;/li&gt;
  &lt;li&gt;Cannot handle dynamic workloads that mix queries and data manipulation statements including inserts, deletes, and updates.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ALICE is less sensitive to data changes&lt;/p&gt;

</description>
        <pubDate>Tue, 14 May 2024 00:00:00 +0000</pubDate>
        <link>https://nlgithubwp.github.io/tech-notebook/journal/213/</link>
        <guid isPermaLink="true">https://nlgithubwp.github.io/tech-notebook/journal/213/</guid>
        
        
        <category>A paper note</category>
        
      </item>
    
  </channel>
</rss>
